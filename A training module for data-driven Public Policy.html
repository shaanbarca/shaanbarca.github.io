<!DOCTYPE html>
<html>
<head>
<title>A training module for data-driven Public Policy.md</title>
<meta http-equiv="Content-type" content="text/html;charset=UTF-8">

<style>
/* https://github.com/microsoft/vscode/blob/master/extensions/markdown-language-features/media/markdown.css */
/*---------------------------------------------------------------------------------------------
 *  Copyright (c) Microsoft Corporation. All rights reserved.
 *  Licensed under the MIT License. See License.txt in the project root for license information.
 *--------------------------------------------------------------------------------------------*/

body {
	font-family: var(--vscode-markdown-font-family, -apple-system, BlinkMacSystemFont, "Segoe WPC", "Segoe UI", "Ubuntu", "Droid Sans", sans-serif);
	font-size: var(--vscode-markdown-font-size, 14px);
	padding: 0 26px;
	line-height: var(--vscode-markdown-line-height, 22px);
	word-wrap: break-word;
}

#code-csp-warning {
	position: fixed;
	top: 0;
	right: 0;
	color: white;
	margin: 16px;
	text-align: center;
	font-size: 12px;
	font-family: sans-serif;
	background-color:#444444;
	cursor: pointer;
	padding: 6px;
	box-shadow: 1px 1px 1px rgba(0,0,0,.25);
}

#code-csp-warning:hover {
	text-decoration: none;
	background-color:#007acc;
	box-shadow: 2px 2px 2px rgba(0,0,0,.25);
}

body.scrollBeyondLastLine {
	margin-bottom: calc(100vh - 22px);
}

body.showEditorSelection .code-line {
	position: relative;
}

body.showEditorSelection .code-active-line:before,
body.showEditorSelection .code-line:hover:before {
	content: "";
	display: block;
	position: absolute;
	top: 0;
	left: -12px;
	height: 100%;
}

body.showEditorSelection li.code-active-line:before,
body.showEditorSelection li.code-line:hover:before {
	left: -30px;
}

.vscode-light.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(0, 0, 0, 0.15);
}

.vscode-light.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(0, 0, 0, 0.40);
}

.vscode-light.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

.vscode-dark.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(255, 255, 255, 0.4);
}

.vscode-dark.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(255, 255, 255, 0.60);
}

.vscode-dark.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

.vscode-high-contrast.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(255, 160, 0, 0.7);
}

.vscode-high-contrast.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(255, 160, 0, 1);
}

.vscode-high-contrast.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

img {
	max-width: 100%;
	max-height: 100%;
}

a {
	text-decoration: none;
}

a:hover {
	text-decoration: underline;
}

a:focus,
input:focus,
select:focus,
textarea:focus {
	outline: 1px solid -webkit-focus-ring-color;
	outline-offset: -1px;
}

hr {
	border: 0;
	height: 2px;
	border-bottom: 2px solid;
}

h1 {
	padding-bottom: 0.3em;
	line-height: 1.2;
	border-bottom-width: 1px;
	border-bottom-style: solid;
}

h1, h2, h3 {
	font-weight: normal;
}

table {
	border-collapse: collapse;
}

table > thead > tr > th {
	text-align: left;
	border-bottom: 1px solid;
}

table > thead > tr > th,
table > thead > tr > td,
table > tbody > tr > th,
table > tbody > tr > td {
	padding: 5px 10px;
}

table > tbody > tr + tr > td {
	border-top: 1px solid;
}

blockquote {
	margin: 0 7px 0 5px;
	padding: 0 16px 0 10px;
	border-left-width: 5px;
	border-left-style: solid;
}

code {
	font-family: Menlo, Monaco, Consolas, "Droid Sans Mono", "Courier New", monospace, "Droid Sans Fallback";
	font-size: 1em;
	line-height: 1.357em;
}

body.wordWrap pre {
	white-space: pre-wrap;
}

pre:not(.hljs),
pre.hljs code > div {
	padding: 16px;
	border-radius: 3px;
	overflow: auto;
}

pre code {
	color: var(--vscode-editor-foreground);
	tab-size: 4;
}

/** Theming */

.vscode-light pre {
	background-color: rgba(220, 220, 220, 0.4);
}

.vscode-dark pre {
	background-color: rgba(10, 10, 10, 0.4);
}

.vscode-high-contrast pre {
	background-color: rgb(0, 0, 0);
}

.vscode-high-contrast h1 {
	border-color: rgb(0, 0, 0);
}

.vscode-light table > thead > tr > th {
	border-color: rgba(0, 0, 0, 0.69);
}

.vscode-dark table > thead > tr > th {
	border-color: rgba(255, 255, 255, 0.69);
}

.vscode-light h1,
.vscode-light hr,
.vscode-light table > tbody > tr + tr > td {
	border-color: rgba(0, 0, 0, 0.18);
}

.vscode-dark h1,
.vscode-dark hr,
.vscode-dark table > tbody > tr + tr > td {
	border-color: rgba(255, 255, 255, 0.18);
}

</style>

<style>
/* Tomorrow Theme */
/* http://jmblog.github.com/color-themes-for-google-code-highlightjs */
/* Original theme - https://github.com/chriskempson/tomorrow-theme */

/* Tomorrow Comment */
.hljs-comment,
.hljs-quote {
	color: #8e908c;
}

/* Tomorrow Red */
.hljs-variable,
.hljs-template-variable,
.hljs-tag,
.hljs-name,
.hljs-selector-id,
.hljs-selector-class,
.hljs-regexp,
.hljs-deletion {
	color: #c82829;
}

/* Tomorrow Orange */
.hljs-number,
.hljs-built_in,
.hljs-builtin-name,
.hljs-literal,
.hljs-type,
.hljs-params,
.hljs-meta,
.hljs-link {
	color: #f5871f;
}

/* Tomorrow Yellow */
.hljs-attribute {
	color: #eab700;
}

/* Tomorrow Green */
.hljs-string,
.hljs-symbol,
.hljs-bullet,
.hljs-addition {
	color: #718c00;
}

/* Tomorrow Blue */
.hljs-title,
.hljs-section {
	color: #4271ae;
}

/* Tomorrow Purple */
.hljs-keyword,
.hljs-selector-tag {
	color: #8959a8;
}

.hljs {
	display: block;
	overflow-x: auto;
	color: #4d4d4c;
	padding: 0.5em;
}

.hljs-emphasis {
	font-style: italic;
}

.hljs-strong {
	font-weight: bold;
}
</style>

<style>
/*
 * Markdown PDF CSS
 */

 body {
	font-family: -apple-system, BlinkMacSystemFont, "Segoe WPC", "Segoe UI", "Ubuntu", "Droid Sans", sans-serif, "Meiryo";
	padding: 0 12px;
}

pre {
	background-color: #f8f8f8;
	border: 1px solid #cccccc;
	border-radius: 3px;
	overflow-x: auto;
	white-space: pre-wrap;
	overflow-wrap: break-word;
}

pre:not(.hljs) {
	padding: 23px;
	line-height: 19px;
}

blockquote {
	background: rgba(127, 127, 127, 0.1);
	border-color: rgba(0, 122, 204, 0.5);
}

.emoji {
	height: 1.4em;
}

code {
	font-size: 14px;
	line-height: 19px;
}

/* for inline code */
:not(pre):not(.hljs) > code {
	color: #C9AE75; /* Change the old color so it seems less like an error */
	font-size: inherit;
}

/* Page Break : use <div class="page"/> to insert page break
-------------------------------------------------------- */
.page {
	page-break-after: always;
}

</style>

<script src="https://unpkg.com/mermaid/dist/mermaid.min.js"></script>
</head>
<body>
  <script>
    mermaid.initialize({
      startOnLoad: true,
      theme: document.body.classList.contains('vscode-dark') || document.body.classList.contains('vscode-high-contrast')
          ? 'dark'
          : 'default'
    });
  </script>
  <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  <script type="text/x-mathjax-config"> MathJax.Hub.Config({ tex2jax: {inlineMath: [['$', '$']]}, messageStyle: "none" });</script>
<h1 id="a-training-module-for-data-driven-public-policy-in-indonesia">A training module for data-driven Public Policy in Indonesia</h1>
<p><strong>Shaan Perlambang (71735892)</strong></p>
<p><strong>shaanbarca@keio.jp</strong></p>
<p><strong>Keio University</strong></p>
<p><strong>Faculty of Policy Management</strong></p>
<p><strong>Supervisor(s):</strong></p>
<p><strong>Mika Kunieda</strong></p>
<p><strong>mkunieda@sfc.keio.ac.jp</strong></p>
<h1 id="table-of-contents">Table of contents</h1>
<ol>
<li><a href="#introduction">Introduction</a>
<ol>
<li>Current situation</li>
<li>What this notebook hopes to address</li>
<li>Notebook Objective</li>
<li>Prerequisite</li>
</ol>
</li>
<li><a href="#How-to-use-this-guide">How to use this guide</a>
<ol>
<li>Framework</li>
<li>Libraries</li>
</ol>
</li>
<li><a href="#starting-the-project">Starting our Project</a>
<ol>
<li>Loading our Data</li>
<li>Functions</li>
<li>Structuring our Project</li>
<li>Initial Exploratory Analysis</li>
<li>Preproccessing our Data</li>
</ol>
</li>
<li><a href="#exploratory-data-analysis">Exploratory data analysis</a>
<ol>
<li>Boxplots</li>
<li>Correlation</li>
<li>Distribution</li>
<li>Importance of Visualizing our Data</li>
</ol>
</li>
<li><a href="#geospatial-visualization-advanced">Geospatial Visualization</a>
<ol>
<li>Joining Data With inconsitent naming</li>
<li>Interactive Maps</li>
</ol>
</li>
<li><a href="#feature-engineering">Feature Engineering</a>
<ol>
<li>Mutual Information</li>
<li>Creating new Features</li>
<li>Dealing with Categorical Data</li>
</ol>
</li>
<li><a href="#modeling-data">Building Models</a>
<ol>
<li>Evaluation Methods</li>
<li>Grid Search/ Hyper-parameter tuning</li>
<li>Diagnosing model performance</li>
<li>Understanding Metrics</li>
<li>Feature Importance</li>
</ol>
</li>
<li><a href="#interpreting-our-model">Interpreting our model</a>
<ol>
<li>SHAP</li>
<li>Data Leakage</li>
<li>Understanding SHAP results</li>
<li>How policy makers interpret our Models</li>
</ol>
</li>
<li><a href="#next-steps-how-to-derive-causality-from-our-results-brief-example">Next Steps</a>
<ol>
<li>Deriving causality</li>
<li>Using Instrumental Variables</li>
</ol>
</li>
<li>Suggestions to the Indonesian Government</li>
</ol>
<h1 id="introduction">Introduction</h1>
<p>The government of Indonesia has launched an initiative called Satu Data Indonesia (SDI) to consolidate data, improve data quality, and enable access to central and local government data. The objectives of the program in its own words are to increase transparency and accountability of the government and help the development of the nation. By leveraging machine learning, we can help realize the goals of the program to help policymakers make data-driven decisions. Badan Pusat Statistik (BPS) Indonesia is a government bureau that collects, surveys, and analyzes data for the government. They are the main entities that provide statistical analysis to help decision-makers to make data-driven policies.</p>
<h2 id="current-situation">Current situation</h2>
<p>The primary focus of the BPS is on sampling or data acquisition and not data analysis[2]. While some data analysis is done, most government ministries, agencies and other forms of organizations need to conduct the analysis independently. As the analysis is being conducted individually, there is yet to be a standardized way to clean, visualize, understand and evaluate the results of their analysis.</p>
<h2 id="what-does-this-notebook-hopes-to-address">What does this notebook hopes to address</h2>
<p>Previous studies on (ML) for public policy in Indonesia have largely focused on prediction/forecasting [4]. Certain implemented models were also ‚Äúblackbox‚Äù methods, where the results were not interpretable.  For this application, policymakers must not only have predictions of future forecasts but also reasoning on how the predictions were derived. While prediction is certainly an important component, it is also important to identify the levers that influence such outcomes. For example, if a model is developed to accurately predict the hospitalization rate in a particular area, we must also know what influences such factors.  Thus, in this domain, we must not only employ models, but also ensure that they are interpretable.
Research has been published on the use of interpretable models in Indonesia. They employed Causal Forest, an ML-based model for Indonesian public healthcare that identifies causal variables [7]. While this was able to successfully derive causal factors, the emphasis on the papers was largely on singular modeling rather than a diverse approach.  Situations related to public policy are often multifaceted problems that require a diverse approach to derive an understanding from different perspectives. Recognizing the use of multiple approaches is something that also needed to be further explored.</p>
<p>Finally, there is a lack of emphasis on understanding visualized data in the current literature. Descriptive statistics are typically included however with current frameworks used in Data Science, ‚ÄúExplanatory Data Analysis‚Äù goes beyond just looking at the mean, mode, median and deviation etc. in tabular fashion. Different types of visualizations are needed to understand the variables involved and to fully grasp the situation. It also helps us to decide which models are the most ideal for the dataset.</p>
<h2 id="notebook-objective">Notebook Objective</h2>
<p>This notebooks aims to serve as a framework for Data science applications for public policy in Indonesia. Specifically, it addresses the main gaps in the current literature by employing data explanatory techniques (1), using an ensemble of models (2) methods to make them interpretable (3), and translating its results to actionable policy recommendations (4). Additionally, it also discusses the end-user‚Äôs involvement and the limitations of ML to avoid negligent use (5). Finally, we will briefly discuss on deriving causality from our results (6).</p>
<p>To fully illustrate this, we analyze the <strong>Penghasilan Asli Daerah / Local Government Revenue or referred to as PAD per Capita</strong> in our dataset of each province in Indonesia. The PAD is income of a city minus subsidy/funds they receive from the central government. The goal of this notebook would then be to see which factors influence a given city‚Äôs income</p>
<p>An extensive list of variables are included, such as geographical location, level of education, and employment to name a few. These variables were represented as continuous, categorical, ordinal, and spatial values.
Dealing with such a diverse dataset allows demonstrations of  how machine learning can be used in public policy. Because the Satu Data Indonesia initiative is still ongoing, the dataset used may not be fully accurate. Thus, the model interpretations and policy recommendations made in this study serve only as an example and to illustrate the framework.</p>
<p>In this notebook we are going to explore datasets from the ministry of finance and population and civil registration agency.</p>
<p><strong>NOTE: Throughout the notebook we will be referring to independent variables and features interchangeably. In Data science or Machine Learning lingo, this means the same thing.</strong></p>
<h2 id="prerequisites">Prerequisites</h2>
<p>There are several prerequisites that need to be fulfilled before following this notebook.</p>
<ol>
<li>A basic understanding of python</li>
</ol>
<p>Understanding variables, functions and using libraries should be enough</p>
<ol>
<li>Experience using commonly used data science libraries</li>
<li>Matplotlib, pandas and NumPy and scikit-learn</li>
<li>A basic understanding of Machine Learning (Statistics, Probability, Linear Algebra)</li>
<li>Experience using Jupyter Notebooks</li>
</ol>
<p>Here are some resources that you can refer to :</p>
<p>Python :</p>
<p><a href="https://www.learnpython.org/">https://www.learnpython.org/</a></p>
<p>Pandas : <a href="https://pandas.pydata.org/docs/user_guide/10min.html">https://pandas.pydata.org/docs/user_guide/10min.html</a></p>
<p>Numpy : <a href="https://numpy.org/doc/stable/user/quickstart.html">https://numpy.org/doc/stable/user/quickstart.html</a></p>
<p>Scikit-learn and machine learning:</p>
<p><a href="https://scikit-learn.org/stable/tutorial/basic/tutorial.html">https://scikit-learn.org/stable/tutorial/basic/tutorial.html</a></p>
<h1 id="how-to-use-this-guide">How to use this guide:</h1>
<p>It is highly recommended to use your own dataset and use this guide as a reference on how to clean, explore, build models and interpret your findings. Sections with advanced in title can be skipped however it is recommended to go through the visualizations to see what insights can be extracted from these steps. This guide will not give an in-depth explanation for each section, that will be outsourced to other resources. Instead, this demonstrates an end to end implementation of DS for Public Policy.</p>
<h2 id="framework">Framework</h2>
<p><img src="A training module for data-driven Public Policy in d18de1689cd5487faa0587ecf949e9ee/Untitled.png" alt="Untitled"></p>
<ol>
<li>Gather data. Data can come from many and different sources, in this notebook we will deal with data from different sources and how to merge them</li>
<li>Transform and clean. Often times, the data that we work with is dirty. This means there are typos. If we do not check the quality of our data, this will lead to spurious regression and false conclusions.</li>
<li>Explore. Exploring our data allows us to see which variables might be interesting, serves as a roadmap on what models to build and also allows to see which data might be false. If we see data that does not make sense, we have to clean them. We will consistently clean and explore data throughout the notebook as we see fit.</li>
<li>Build models and Analyze. Ultimately, in this notebook our goal is to use Machine Learning models. There are plenty of examples of using linear regression and we will use some linear regression in this notebook however we would also like to explore how to use state of the art models! Note: A lot of utility can be derived from linear regression. When possible always use simpler models.</li>
<li>Communicate our results. Ultimately, the goal of our notebook is to interpret our finding and articulate them to policy recommendations. Often we instances where practitioners stop after they have built models and focusing too much on how to improve performance. While an accurate model is no doubt important, for policy makers they want to know what actions to take. This is why we should spend our time on interpreting our results.</li>
</ol>
<h2 id="libraries">Libraries</h2>
<p>These are the python libraries that will be used to allow us to conduct the analysis</p>
<p>Installing libraries can be tricky. Navigate to the <strong>Appendix</strong> below and read the <strong>Reproducible environments</strong> section.</p>
<pre class="hljs"><code><div><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt
<span class="hljs-keyword">import</span> matplotlib <span class="hljs-keyword">as</span> mlp
<span class="hljs-keyword">import</span> seaborn <span class="hljs-keyword">as</span> sns
<span class="hljs-keyword">from</span> matplotlib <span class="hljs-keyword">import</span> rcParams
<span class="hljs-keyword">import</span> geopandas <span class="hljs-keyword">as</span> gpd
<span class="hljs-comment">#----------------------- Data analysis libs</span>

<span class="hljs-keyword">from</span> sklearn.feature_selection <span class="hljs-keyword">import</span> mutual_info_regression
<span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> GridSearchCV
<span class="hljs-keyword">import</span> xgboost <span class="hljs-keyword">as</span> xgb
<span class="hljs-keyword">from</span> sklearn <span class="hljs-keyword">import</span> metrics
<span class="hljs-keyword">from</span> sklearn.preprocessing <span class="hljs-keyword">import</span> LabelEncoder, StandardScaler, OrdinalEncoder
<span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> classification_report,confusion_matrix
<span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> multilabel_confusion_matrix
<span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> plot_confusion_matrix

<span class="hljs-comment">#-------------- We want to set the figure size here </span>
sns.set(rc={<span class="hljs-string">'figure.figsize'</span>:(<span class="hljs-number">20</span>,<span class="hljs-number">10</span>)})
plt.style.use(
    <span class="hljs-string">"https://github.com/aeturrell/coding-for-economists/raw/main/plot_style.txt"</span>
)
rcParams[<span class="hljs-string">'figure.figsize'</span>] = <span class="hljs-number">20</span>,<span class="hljs-number">10</span>

<span class="hljs-comment">#----------------- Pandas settings</span>
pd.set_option(<span class="hljs-string">'display.max_columns'</span>, <span class="hljs-literal">None</span>)
pd.set_option(<span class="hljs-string">'display.max_rows'</span>, <span class="hljs-literal">None</span>)
</div></code></pre>
<h1 id="starting-the-project">Starting the Project</h1>
<h2 id="loading-the-dataset">Loading the dataset</h2>
<p>There are several problems that will occur frequently when dealing with datasets.</p>
<ul>
<li>Data inputed incorrectly</li>
<li>Inconsistent naming, this is especially difficult when dealing with multiple datasets</li>
<li>Modifying data to ensure they are in the correct format</li>
</ul>
<p>Throughout the notebook, we will be dealing with all these problems and clean them along the way</p>
<pre class="hljs"><code><div>df = pd.read_excel(<span class="hljs-string">'normalized_gp2_data.xlsx'</span>)
df.head()
</div></code></pre>
<p><img src="A training module for data-driven Public Policy in d18de1689cd5487faa0587ecf949e9ee/Screen_Shot_2022-07-15_at_13.17.38.png" alt="Screen Shot 2022-07-15 at 13.17.38.png"></p>
<pre class="hljs"><code><div>df.shape
</div></code></pre>
<p><code>(507, 91)</code></p>
<p>91 columns ! Thats quite a lot of features. We should explore how the relationship of our different features with each other and see which columns are most important.</p>
<h2 id="functions">Functions</h2>
<p>These are the 2 functions that we will use. As you can imagine calculating summary statistics or outliers is something that will be done multiple times</p>
<pre class="hljs"><code><div><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">descriptive_table</span><span class="hljs-params">(data)</span>:</span>
	<span class="hljs-string">"""Outputs descriptive statistics of our DataFrame
	
	
	 Args:
         data (DataFrame): Pandas DataFrame to calculate summary statistics
	
	 Returns:
         sum_table(DataFrame): Pandas DataFrame summary statistics transposed
	
	  """</span>
	  sum_table = data.describe().round(<span class="hljs-number">1</span>)
	  sum_table = sum_table.T
	  <span class="hljs-keyword">return</span> sum_table

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">outlier_data</span><span class="hljs-params">(data,col)</span>:</span>
    <span class="hljs-string">"""outlier value of particular data frame column

    Args:
        data (DataFrame): Dataframe to calculate outlier from
        col (str): column to calculate outlier from

    Returns:
        outlier (float): outlier value of a particular column

    """</span>
    percentiles = descriptive_table(data[[*col]])
    q3 = percentiles[<span class="hljs-string">'75%'</span>]
    q1 = percentiles[<span class="hljs-string">'25%'</span>]
    iqr = q3-q1
    outlier = q3 + (<span class="hljs-number">1.5</span>*iqr)
    <span class="hljs-keyword">return</span> float(outlier)
</div></code></pre>
<p>To reuse these functions multiple times, we can create a utility library with these functions (or more) within it instead of defining the function for every notebook/script. Store your notebook/script and utilities in the same path</p>
<pre class="hljs"><code><div>üìÅ project
‚îÇ‚îÄ‚îÄmain.ipynb or main.py
‚îÇ‚îÄ‚îÄutilities.py
</div></code></pre>
<p>Import them to your notebook/script</p>
<pre class="hljs"><code><div><span class="hljs-keyword">from</span> utilities <span class="hljs-keyword">import</span> descriptive_table, outlier_data
</div></code></pre>
<h2 id="initial-exploratory-data-analysis">Initial Exploratory data analysis</h2>
<h3 id="heat-map">Heat map</h3>
<p>By using a heat map, we can see how each variable correlates with each other. We can also see which variables are correlated to our target variable (PAD per Capita).</p>
<pre class="hljs"><code><div>plt.figure(figsize=(<span class="hljs-number">100</span>, <span class="hljs-number">50</span>))
mask = np.triu(np.ones_like(df.iloc[:,<span class="hljs-number">3</span>:].corr(), dtype=bool))

heatmap =sns.heatmap(df.iloc[:,<span class="hljs-number">3</span>:].corr() ,mask=mask, vmin=<span class="hljs-number">-1</span>,vmax=<span class="hljs-number">1</span>,annot=<span class="hljs-literal">True</span>,cmap=<span class="hljs-string">'BrBG'</span>)
heatmap.set_title(<span class="hljs-string">'Correlation Heatmap'</span>, fontdict={<span class="hljs-string">'fontsize'</span>:<span class="hljs-number">18</span>}, pad=<span class="hljs-number">12</span>);
plt.savefig(<span class="hljs-string">'heatmap.png'</span>, dpi=<span class="hljs-number">300</span>, bbox_inches=<span class="hljs-string">'tight'</span>)
</div></code></pre>
<p><img src="A training module for data-driven Public Policy in d18de1689cd5487faa0587ecf949e9ee/Unknown.png" alt="Unknown.png"></p>
<p>From this heat map we can see variables such as Agama and Kepercayaan or Tenaga kesehatan has no correlation to the target variable. This could also be that these independent variables has not been inputed appropriately.</p>
<h3 id="multicollinearity">Multicollinearity</h3>
<p>Depending on the model we will be using, we should remove variables that are highly correlated. The main reason multicollinearity is a problem is it makes interpreting our models much more difficult. It makes it difficult to isolate how a variable impacts our model and also makes our model unstable. We will explore this in detail in the pre-processing section</p>
<h3 id="remove-features-that-have-no-correlation">Remove features that have no correlation</h3>
<p>In this instance, all of the columns that have no correlation (grey) in relation to the target variable will be dropped. This of course depends on our knowledge of the dataset too. There are certain variables that may not be useful on their own but important once we combine it with other variables. In this case, it might not be wise to completely drop them. In the appendix, there will be other alternatives on how to deal reducing dimensionality of our data</p>
<pre class="hljs"><code><div>df = df.drop(columns=[<span class="hljs-string">'Agama dan Kepercayaan'</span>,<span class="hljs-string">'Tenaga Kesehatan'</span>,<span class="hljs-string">'Kepercayaan terhadap Tuhan YME'</span>,<span class="hljs-string">'Unnamed: 0'</span>,<span class="hljs-string">'f4_18_tahun_pendidikan_khusus'</span>,
                      <span class="hljs-string">'f5_6_tahun_paud'</span>,<span class="hljs-string">'f7_12_tahun_sd'</span>,<span class="hljs-string">'f12_15_tahun_smp'</span>,<span class="hljs-string">'f16_18_tahun_sma'</span>,<span class="hljs-string">'lahir_thn4'</span>,<span class="hljs-string">'lahir_thn5'</span>,<span class="hljs-string">'lahir_thn6'</span>,<span class="hljs-string">'lahir_seb4'</span>,
                      <span class="hljs-string">'lahir_seb5'</span>,<span class="hljs-string">'lahir_seb6'</span>,<span class="hljs-string">'Usia sekolah 3-4 thn'</span>,<span class="hljs-string">'Usia sekolah 5 thn'</span>,<span class="hljs-string">'Usia sekolah 6-11 thn'</span>,<span class="hljs-string">'Usia sekolah 12-24 thn'</span>,<span class="hljs-string">'Usia sekolah 15-17 thn'</span>,<span class="hljs-string">'Usia sekolah 18-22 thn'</span>],axis=<span class="hljs-number">1</span>)
</div></code></pre>
<h3 id="descriptive-statistics">Descriptive statistics</h3>
<p>Descriptive statistics useful to see if our data is reliable. In this particular table we have the count, mean, std, min max and percentiles.From here we can see if the ranges of our data make sense. We can easily cross check our data with other data sources to verify its validity. For example, we can easily check that Jakarta is the most populated city with a population of around 11 million. If any city/regency exceeds this value, then it is most likely erroneous data. There are other ways to check for outliers but this is the most basic way. For instance, we know that the population in Indonesia is around 275 million, if the average or max population of our Jumlah Penduduk( Number of population) exceeds this amount or is close to this amount, we know that there is something wrong with our data.</p>
<pre class="hljs"><code><div>summary_stats_all = descriptive_table(df)
summary_stats_all.head(<span class="hljs-number">5</span>)
</div></code></pre>
<p><img src="A training module for data-driven Public Policy in d18de1689cd5487faa0587ecf949e9ee/Screen_Shot_2022-07-20_at_15.39.48.png" alt="Screen Shot 2022-07-20 at 15.39.48.png"></p>
<h2 id="preprocessing-data">Preprocessing data</h2>
<h3 id="feature-scaling">Feature scaling</h3>
<p>Since we are interested in the Income per Capita of a City/Regency, we must also get the percentages of features instead of raw value. For example, if we had 2 cities, City A and City B and they both have 10,000 people who have attained a bachelors degree if City A has a population of 100,000 while City B has a population of 1,000,000 we know that in City A, 10% of the population are well educated while in City B only 1% are well educated. By not scaling them to percentages, we are not giving a clear picture of the whole situation.</p>
<p>As we are calculating income per capita, the same logic follows when scaling our data:
with percentage of those with a bachelors education for example it would be :</p>
<p>Percentage of those with a bachelors education = number of people with a bachelors degree/ population of city</p>
<pre class="hljs"><code><div><span class="hljs-comment"># calculate population density ourselves</span>
df[<span class="hljs-string">'population_density'</span>] = df[<span class="hljs-string">'Jumlah Penduduk'</span>] / df[<span class="hljs-string">'Luas Wilayah (km2)'</span>] <span class="hljs-comment"># results were different then the one calculated by the government</span>
df[<span class="hljs-string">'Percentage_married'</span>] = df[<span class="hljs-string">'Kawin'</span>] / df[<span class="hljs-string">'Jumlah Penduduk'</span>] 
df[<span class="hljs-string">'Percentage_divorced_alive'</span>] = df[<span class="hljs-string">'Cerai Hidup'</span>] / df[<span class="hljs-string">'Jumlah Penduduk'</span>]  
df[<span class="hljs-string">'Percentage_divorced_death'</span>] = df[<span class="hljs-string">'Cerai Mati'</span>] / df[<span class="hljs-string">'Jumlah Penduduk'</span>]

<span class="hljs-comment"># remove redundant features</span>

df = df.drop(columns=[<span class="hljs-string">'Cerai Hidup'</span>,<span class="hljs-string">'Cerai Mati'</span>,<span class="hljs-string">'Kawin'</span>,<span class="hljs-string">'Belum Kawin'</span>,<span class="hljs-string">'Kepadatan Penduduk'</span>],axis=<span class="hljs-number">1</span>)
</div></code></pre>
<h2 id="removing-features">Removing features</h2>
<p>While additional features/independent variables can help us making better predictions, often multi- collinear features or data that is not relevant to our model may harm it. The reason for this is our model will start overfitting. Overfitting occurs when our model starts memorizing our dataset instead of identifying patterns. Our model performs well in the 'training set' the sample its uses to find the optimal parameters but struggles to generalize in the 'test set'.</p>
<p>There are multiple ways to approach this. Depending on our knowledge of the data we can simply remove features we know are not important. If not there are other methods to remove or even combine features that will be explored on later.</p>
<pre class="hljs"><code><div>df = df.drop(columns=[<span class="hljs-string">'Cerai Hidup'</span>,<span class="hljs-string">'Cerai Mati'</span>,<span class="hljs-string">'Kawin'</span>,<span class="hljs-string">'Belum Kawin'</span>,<span class="hljs-string">'Kepadatan Penduduk'</span>],axis=<span class="hljs-number">1</span>)
</div></code></pre>
<h1 id="exploratory-data-analysis">Exploratory data analysis</h1>
<p>Exploratory data analysis (EDA) summarizes the datasets through summary statistics and visual representations. This allows us to recognize patterns, identify outliers, and understand the types of data that we are dealing with. It helps us build the intuition of our dataset by selecting how we should clean our data, what potential problems we may face when modeling our data, what types of analysis we can conduct, what models we should select, and help those with less technical knowledge better understand our data.</p>
<p>Our dataset can be visualized in multiple ways. These are divided into three types of visualization analysis.</p>
<ol>
<li>Univariate analysis was performed to visualize the distribution of the data. This is done when analyzing a single variable.</li>
<li>Bivariate analysis is done to visualize the relationship of two variables.</li>
<li>Multivariate analysis is done when there are two or more.</li>
</ol>
<p>Common visualizations are histograms, box plots and scatterplots.</p>
<h2 id="box-plots">Box plots</h2>
<p>Looking at data from tables to see and check which are anomalous or potentially erroneous would take a long time. A solution to that is by utilizing box plots to detect outliers. We can identify outliers by using the function we defined above. We need to ensure the outliers are actually valid.</p>
<p>Outliers should not be removed immediately as they can give us useful insights on why they occur. Later on in the notebook, we will try to understand why they occur</p>
<pre class="hljs"><code><div>plt.figure(figsize=(<span class="hljs-number">20</span>, <span class="hljs-number">10</span>))
ax = sns.boxplot(x=df[<span class="hljs-string">'PAD per Capita'</span>])
ax.set_title(<span class="hljs-string">"PAD per Capita"</span>, loc=<span class="hljs-string">"center"</span>)
</div></code></pre>
<p><img src="A training module for data-driven Public Policy in d18de1689cd5487faa0587ecf949e9ee/Screen_Shot_2022-07-15_at_13.21.07.png" alt="Screen Shot 2022-07-15 at 13.21.07.png"></p>
<p>We can see quite a few outliers when we visualize our data using box plots, lets see what those outliers are</p>
<pre class="hljs"><code><div>pad_outlier = outlier_data(df,<span class="hljs-string">'PAD per Capita'</span>)

df[df[<span class="hljs-string">'PAD per Capita'</span>]&gt; pad_outlier][[<span class="hljs-string">'Kabupaten/Kota'</span>,<span class="hljs-string">'PAD per Capita'</span>]
].sort_values(
by=<span class="hljs-string">'PAD per Capita'</span>,ascending=<span class="hljs-literal">False</span>)
</div></code></pre>
<p><img src="A training module for data-driven Public Policy in d18de1689cd5487faa0587ecf949e9ee/Screen_Shot_2022-07-15_at_13.22.45.png" alt="Screen Shot 2022-07-15 at 13.22.45.png"></p>
<p>We can then check one by one if there are any cities with data thats are falsely inputed. While this process can still be tedious, this narrows the search for cities that might have data that was falsely inputed.</p>
<h2 id="correlation">Correlation</h2>
<p>Now we want to see which of our independent variables are linearly correlated with our independent variable. When we use our heat map before, we saw how they were correlated with each other, similar to a correlation matrix. Now, we can zoom in and see how they are correlated with PAD per Capita</p>
<pre class="hljs"><code><div>highest_pos_corr_pad = df.corr()[[<span class="hljs-string">'PAD per Capita'</span>]].sort_values(by=<span class="hljs-string">'PAD per Capita'</span>,ascending=<span class="hljs-literal">False</span>).head(<span class="hljs-number">10</span>)
highest_pos_corr_pad[<span class="hljs-string">'category'</span>] = highest_pos_corr_pad.index
highest_pos_corr_pad.head()
ax = sns.barplot(x=<span class="hljs-string">"category"</span>, y=<span class="hljs-string">"PAD per Capita"</span>, data=highest_pos_corr_pad)
ax.set_xticklabels(labels = highest_pos_corr_pad.category,rotation=<span class="hljs-number">45</span>)
ax.set_title(<span class="hljs-string">"Highest correlation value to PAD per Capita"</span>, loc=<span class="hljs-string">"center"</span>)
</div></code></pre>
<p><img src="A training module for data-driven Public Policy in d18de1689cd5487faa0587ecf949e9ee/Screen_Shot_2022-07-15_at_13.24.19.png" alt="Screen Shot 2022-07-15 at 13.24.19.png"></p>
<p>We see that population density and variables pertaining level of education are positively correlated to a cities PAD per Capita</p>
<pre class="hljs"><code><div>lowest_pos_corr_pad = df.corr()[[<span class="hljs-string">'PAD per Capita'</span>]].sort_values(by=<span class="hljs-string">'PAD per Capita'</span>,ascending=<span class="hljs-literal">False</span>).tail(<span class="hljs-number">10</span>)
lowest_pos_corr_pad[<span class="hljs-string">'category'</span>] = lowest_pos_corr_pad.index
<span class="hljs-comment">#lowest_pos_corr_pad.head()</span>
ax = sns.barplot(x=<span class="hljs-string">"category"</span>, y=<span class="hljs-string">"PAD per Capita"</span>, data=lowest_pos_corr_pad)
ax.set_xticklabels(labels = lowest_pos_corr_pad.category,rotation=<span class="hljs-number">45</span>)
ax.set_title(<span class="hljs-string">"Highest negative correlation value to PAD per Capita"</span>, loc=<span class="hljs-string">"center"</span>)
</div></code></pre>
<p>We can also visualize which variables are negatively correlated to our independent variable</p>
<p><img src="A training module for data-driven Public Policy in d18de1689cd5487faa0587ecf949e9ee/Screen_Shot_2022-07-15_at_13.26.05.png" alt="Screen Shot 2022-07-15 at 13.26.05.png"></p>
<p>Now that we have identified which variables have the highest correlation value, let visualize their distributions</p>
<h2 id="distributions">Distributions</h2>
<p>We will first start by plotting the distribution of the PAD per Capita</p>
<pre class="hljs"><code><div>ax = sns.histplot(data=df,x=<span class="hljs-string">"PAD per Capita"</span>,kde=<span class="hljs-literal">True</span>)
</div></code></pre>
<p><img src="A training module for data-driven Public Policy in d18de1689cd5487faa0587ecf949e9ee/Screen_Shot_2022-07-15_at_13.29.11.png" alt="Screen Shot 2022-07-15 at 13.29.11.png"></p>
<p>By plotting the distribution, we see that we have a positively skewed distribution. This means the median of our data is less than the mean. What this is telling us is that a few cities/regencies in Indonesia have an Income per Capita that is much higher than the rest that influences our mean to be greater than the median. Or another way of saying it is The income distribution in Indonesia is imbalanced.</p>
<p>Let us then plot the distributions of other variables that show a strong correlation to the PAD per Capita</p>
<pre class="hljs"><code><div>ax = sns.histplot(data=df,x=<span class="hljs-string">"Bachelors_percentage"</span>,kde=<span class="hljs-literal">True</span>)
</div></code></pre>
<p><img src="A training module for data-driven Public Policy in d18de1689cd5487faa0587ecf949e9ee/Screen_Shot_2022-07-15_at_13.33.55.png" alt="Screen Shot 2022-07-15 at 13.33.55.png"></p>
<pre class="hljs"><code><div>ax = sns.histplot(data=df,x=<span class="hljs-string">"highervocational_school_percentage"</span>,kde=<span class="hljs-literal">True</span>)
</div></code></pre>
<p><img src="A training module for data-driven Public Policy in d18de1689cd5487faa0587ecf949e9ee/Screen_Shot_2022-07-15_at_13.34.28.png" alt="Screen Shot 2022-07-15 at 13.34.28.png"></p>
<pre class="hljs"><code><div>ax = sns.histplot(data=df,x=<span class="hljs-string">"high_school_percentage"</span>,kde=<span class="hljs-literal">True</span>)
</div></code></pre>
<p><img src="A training module for data-driven Public Policy in d18de1689cd5487faa0587ecf949e9ee/Screen_Shot_2022-07-15_at_13.36.38.png" alt="Screen Shot 2022-07-15 at 13.36.38.png"></p>
<pre class="hljs"><code><div>ax = sns.histplot(data=df,x=<span class="hljs-string">"population_density"</span>,kde=<span class="hljs-literal">True</span>)
</div></code></pre>
<p><img src="A training module for data-driven Public Policy in d18de1689cd5487faa0587ecf949e9ee/Screen_Shot_2022-07-15_at_13.37.27.png" alt="Screen Shot 2022-07-15 at 13.37.27.png"></p>
<p>After we plot all the variables that are strongly correlated to the PAD per Capita we see they all have a skewed distribution. Intuitively this makes sense with as we know that majority of the income is concentrated in certain cities, based on our initial visualization, and so do the factors that positively contribute to a city‚Äôs Income.</p>
<h2 id="importance-of-visualizing-our-data">Importance of Visualizing our Data</h2>
<p>After displaying the correlation, we want to plot it to ensure that our variables are indeed correlated.</p>
<h3 id="anscombes-quartet">Anscombe‚Äôs quartet</h3>
<p>This phenomena occurs when a group of datasets have descriptive statistics that are nearly identical to each other and yet are very different when we visualize them</p>
<p><img src="A training module for data-driven Public Policy in d18de1689cd5487faa0587ecf949e9ee/Untitled 1.png" alt="data source : matplotlib[9]">
data source : matplotlib[9]</p>
<p>From the figure above can see despite the 4 separate datasets having identical descriptive statistics, yet, they are actually different.</p>
<p>Now we can plot a scatterplot and add a regression line to see if there actually is a relationship</p>
<pre class="hljs"><code><div>ax = sns.regplot(x=<span class="hljs-string">"high_school_percentage"</span>, y=<span class="hljs-string">"PAD per Capita"</span>, data=df)
ax.set_title(<span class="hljs-string">"Regression of Highschool Percentage to PAD per Capita"</span>, loc=<span class="hljs-string">"center"</span>)
</div></code></pre>
<p><img src="A training module for data-driven Public Policy in d18de1689cd5487faa0587ecf949e9ee/Screen_Shot_2022-07-15_at_13.42.27.png" alt="Screen Shot 2022-07-15 at 13.42.27.png"></p>
<pre class="hljs"><code><div>ax = sns.regplot(x=<span class="hljs-string">"highervocational_school_percentage"</span>, y=<span class="hljs-string">"PAD per Capita"</span>, data=df)
ax.set_title(<span class="hljs-string">"Regression of Vocational school Percentage to PAD per Capita"</span>, loc=<span class="hljs-string">"center"</span>)
</div></code></pre>
<p><img src="A training module for data-driven Public Policy in d18de1689cd5487faa0587ecf949e9ee/Screen_Shot_2022-07-15_at_13.42.56.png" alt="Screen Shot 2022-07-15 at 13.42.56.png"></p>
<pre class="hljs"><code><div>ax = sns.regplot(x=<span class="hljs-string">"Bachelors_percentage"</span>, y=<span class="hljs-string">"PAD per Capita"</span>, data=df)
ax.set_title(<span class="hljs-string">"Regression of Bachelors Percentage to PAD per Capita"</span>, loc=<span class="hljs-string">"center"</span>)
</div></code></pre>
<p><img src="A training module for data-driven Public Policy in d18de1689cd5487faa0587ecf949e9ee/Screen_Shot_2022-07-15_at_13.44.11.png" alt="Screen Shot 2022-07-15 at 13.44.11.png"></p>
<p>We can use subplots to make it more our regression more compact. This also prevent us from repeating our code.</p>
<pre class="hljs"><code><div><span class="hljs-comment"># using subplots</span>
plt.figure(figsize=(<span class="hljs-number">15</span>, <span class="hljs-number">12</span>))
plt.subplots_adjust(hspace=<span class="hljs-number">0.5</span>)
plt.suptitle(<span class="hljs-string">"Linear regression between variables"</span>, fontsize=<span class="hljs-number">18</span>, y=<span class="hljs-number">0.95</span>)

<span class="hljs-comment"># loop through our list of variables</span>
<span class="hljs-keyword">for</span> n, col <span class="hljs-keyword">in</span> enumerate(highest_correlation_list):
    <span class="hljs-comment"># add a new subplot iteratively</span>
    ax = plt.subplot(<span class="hljs-number">3</span>, <span class="hljs-number">2</span>, n + <span class="hljs-number">1</span>)

    sns.regplot(x=col, y=<span class="hljs-string">"PAD per Capita"</span>, data=df,ax=ax)

    <span class="hljs-comment"># chart formatting</span>
    ax.set_xlabel(col)
plt.savefig(<span class="hljs-string">'subtplot_reg.png'</span>)
</div></code></pre>
<p><img src="A training module for data-driven Public Policy in d18de1689cd5487faa0587ecf949e9ee/subtplot_reg.png" alt="subtplot_reg.png"></p>
<p>An alternative to plotting the regression is by using  pandas‚Äôs¬†<code>scattermatrix()</code></p>
<pre class="hljs"><code><div>highest_pos_corr_pad = df.corr()[[<span class="hljs-string">'PAD per Capita'</span>]].sort_values(by=<span class="hljs-string">'PAD per Capita'</span>,ascending=<span class="hljs-literal">False</span>).head(<span class="hljs-number">6</span>)
highest_correlation_list = list(highest_pos_corr_pad.index)
highest_correlation_list <span class="hljs-comment"># list of cols with the highest correlation</span>
scatter_matrix(df[highest_correlation_list],figsize=(<span class="hljs-number">20</span>,<span class="hljs-number">16</span>))
plt.show()
</div></code></pre>
<p><img src="A training module for data-driven Public Policy in d18de1689cd5487faa0587ecf949e9ee/scattermatrix2.png" alt="scattermatrix2.png"></p>
<p>By doing so, we can see the relationship of each variable not only to our dependent variable but also to other variables. The advantage of this to a heat map is we can actually see how the relationships look like when they are visualized.</p>
<h3 id="querying-our-data">Querying our data</h3>
<p>Based on the our figures, we see that level of education (bachelors, vocational, high school etc) strongly correlate with a cities income. ****Let see which cities that have high income and yet have a level of education that is below average.</p>
<p>We will first see the percentile values of our different features</p>
<pre class="hljs"><code><div>descriptive_table(df[[<span class="hljs-string">'PAD per Capita'</span>,<span class="hljs-string">'Bachelors_percentage'</span>,
<span class="hljs-string">'highervocational_school_percentage'</span>,<span class="hljs-string">'high_school_percentage'</span>,
<span class="hljs-string">'population_density'</span>]])
</div></code></pre>
<p><img src="A training module for data-driven Public Policy in d18de1689cd5487faa0587ecf949e9ee/Screen_Shot_2022-07-20_at_02.41.23.png" alt="Screen Shot 2022-07-20 at 02.41.23.png"></p>
<p>Now we just extract the values below the 50th percentile and specify the multiple conditions to filter the results</p>
<pre class="hljs"><code><div>low_educ_high_income = df.loc[(df[<span class="hljs-string">'PAD per Capita'</span>]&gt;=<span class="hljs-number">100</span>) &amp; 
(df[<span class="hljs-string">'Bachelors_percentage'</span>]&lt; <span class="hljs-number">3</span>) &amp; (df[<span class="hljs-string">'high_school_percentage'</span>] &lt; <span class="hljs-number">18</span>) &amp; 
(df[<span class="hljs-string">'highervocational_school_percentage'</span>] &lt; <span class="hljs-number">1</span>) &amp; 
(df[<span class="hljs-string">'population_density'</span>] &lt; <span class="hljs-number">139.8</span>)][[<span class="hljs-string">'Provinsi'</span>,<span class="hljs-string">'Kabupaten/Kota'</span>,
<span class="hljs-string">'Bachelors_percentage'</span>,<span class="hljs-string">'high_school_percentage'</span>,
<span class="hljs-string">'highervocational_school_percentage'</span>]]
low_educ_high_income
</div></code></pre>
<p><img src="A training module for data-driven Public Policy in d18de1689cd5487faa0587ecf949e9ee/Screen_Shot_2022-07-19_at_23.42.36.png" alt="Screen Shot 2022-07-19 at 23.42.36.png"></p>
<p>These are just some of the cities that have a level of education that is below average (some of the cities here have level of education within the 25th percentile) and yet they have income above the 75th percentile. This does not add up with our correlation results and the scatter plots. There must be a variable that we are not accounting for.</p>
<p>If we search the sources of income from the cities above, it is mostly from natural resources. We do not have data for natural resources in our dataset and this is why certain cities have high income despite their having low levels of education. There might also be other factors that we did not take into account to which we will explore in the modeling section.</p>
<h1 id="geospatial-visualization-advanced">Geospatial visualization (Advanced)</h1>
<p>This section is optional and can be skipped. There is however a data cleaning section which shows how to join data from multiple databases with inconsistent naming that is worth reading.</p>
<p>To get a clearer picture of our data, as we are dealing with the income of different cities in Indonesia, we want to see how the distribution of income looks from a spatial perspective. Prior to this, we visualized the income per capita by plotting the distribution. While it was useful to see that the income was positively skewed, It did not tell us where in Indonesia the cities that had high income were located. Perhaps their spatial location could give us a clue on why they had such high incomes. Using GeoPandas, we can load, manipulate and visualize spatial data.</p>
<p><strong>Note</strong>: The specific details of geospatial visualization is beyond the scope of this guide. In fact, we could dedicate a whole separate guide for geospatial visualization alone. Rather, we will use geospatial visualization here to show how this could be useful for public policy.</p>
<p>In order to spatially visualize our data, we must get the .shp file of our area of interest. A shp file contains vector features of our area of interest. In our case these are all the shapes of all the villages in Indonesia. This data at the village level of a country is publicly available in the geospatial information agency.</p>
<p>We are going to be combining the shp file, with our current dataset which has information containing income, level education, etc.</p>
<p>A separate guide can be dedicated for geospatial visualization and analysis. For a more comprehensive guide refer to : <a href="https://geopandas.org/en/stable/docs.html">https://geopandas.org/en/stable/docs.html</a></p>
<h2 id="load-dataset">Load dataset</h2>
<pre class="hljs"><code><div>
full_data = gpd.read_file(<span class="hljs-string">'Batas Desa terbaru Maret 2020/Batas Desa terbaru Maret 2020.shp'</span>)
city_df = pd.read_excel(<span class="hljs-string">'normalized_gp2_data.xlsx'</span>) <span class="hljs-comment"># reload df</span>
</div></code></pre>
<p>Subset the data extracting only columns of interest. In the data that we are using, we have data at the village level, since we are only interest in at the city level, we find the column containing the city name and also the geometry of each village. Since the naming depend on the author of the dataset, you should read the documentation ahead of time to identify these columns</p>
<pre class="hljs"><code><div>geo_df = geo_df[[<span class="hljs-string">'WADMKK'</span>,<span class="hljs-string">'geometry'</span>]]   <span class="hljs-comment"># subsetting columns</span>
geo_df = geo_df.dropna(axis=<span class="hljs-number">0</span>) <span class="hljs-comment"># remove missing data</span>
</div></code></pre>
<h2 id="how-to-aggregate-spatial-data">How to aggregate spatial data</h2>
<p>Since the data that we have is at the village level, we must aggregate it to make it city level. The The¬†<code>dissolve()</code> function does just that.</p>
<pre class="hljs"><code><div>geo_df[<span class="hljs-string">'geometry'</span>] = geo_df.buffer(<span class="hljs-number">0.01</span>) <span class="hljs-comment"># prevents overlap</span>
geo_df = geo_df.dissolve(by=<span class="hljs-string">'WADMKK'</span>) <span class="hljs-comment"># group our data points by city</span>
geo_df.plot()
geo_df[<span class="hljs-string">"kota"</span>] = geo_df.index <span class="hljs-comment"># we want then to extract the city name</span>
</div></code></pre>
<h2 id="joining-data-with-inconsistent-naming">Joining Data With inconsistent naming</h2>
<p>When dealing with datasets with multiple sources, one of the biggest challenges is the inconsistency in naming. Right now, we want to combine our dataset containing the income per capita of each city and the dataset containing the location. The city names however are not consistent with each other. How can we then combine our datasets together ?</p>
<p>Fortunately a library called fuzzywuzzy allows us to join data based on names the are <strong>similar</strong> to each other. This can save us days or even weeks !</p>
<p>Refer to this guide to use fuzzywuzzy : <a href="https://www.geeksforgeeks.org/fuzzywuzzy-python-library/">https://www.geeksforgeeks.org/fuzzywuzzy-python-library/</a></p>
<pre class="hljs"><code><div><span class="hljs-comment"># merge data</span>
city_df[<span class="hljs-string">'key'</span>]=city_df[<span class="hljs-string">'Kabupaten/Kota'</span>].apply
(<span class="hljs-keyword">lambda</span> x : [process.extract(x, geo_df[<span class="hljs-string">'kota'</span>], limit=<span class="hljs-number">1</span>)][<span class="hljs-number">0</span>][<span class="hljs-number">0</span>][<span class="hljs-number">0</span>])
merged_df = city_df.merge(geo_df,left_on=<span class="hljs-string">'key'</span>,right_on=<span class="hljs-string">'kota'</span>)
merged_df = gpd.GeoDataFrame(merged_df, crs=<span class="hljs-string">"EPSG:4326"</span>, geometry=<span class="hljs-string">'geometry'</span>) <span class="hljs-comment"># convert our data to geopandas data frame</span>
</div></code></pre>
<p>The code in the first line is telling our program to find strings that are similar to each other between these 2 columns from our 2 different datasets.</p>
<h2 id="exploring-our-spatial-data">Exploring our spatial data</h2>
<p>In the previous section, we saw that level of education (Bachelors, Vocational, High school ) were the most important factors to determine a cities income level, we want to see how this holds up by visualizing which areas have the highest level of education and then overlaying our data with the percentage of the population with a bachelors degree. Again, we select only our columns of interest and we can spatially visualize our data.</p>
<pre class="hljs"><code><div>merged_df = merged_df[[<span class="hljs-string">'Kabupaten/Kota'</span>,<span class="hljs-string">'PAD per Capita'</span>,<span class="hljs-string">'geometry'</span>,<span class="hljs-string">'Bachelors_percentage'</span>,<span class="hljs-string">'highervocational_school_percentage'</span>,<span class="hljs-string">'high_school_percentage'</span>]]
merged_df[<span class="hljs-string">'Kabupaten/Kota'</span>] = merged_df[<span class="hljs-string">'Kabupaten/Kota'</span>].astype(str)
</div></code></pre>
<pre class="hljs"><code><div><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">truncate_colormap</span><span class="hljs-params">(cmap, minval=<span class="hljs-number">0.0</span>, maxval=<span class="hljs-number">1.0</span>, n=<span class="hljs-number">100</span>)</span>:</span>
    new_cmap = colors.LinearSegmentedColormap.from_list(
        <span class="hljs-string">'trunc({n},{a:.2f},{b:.2f})'</span>.format(n=cmap.name, a=minval, b=maxval),
        cmap(np.linspace(minval, maxval, n)))
    <span class="hljs-keyword">return</span> new_cmap

arr = np.linspace(<span class="hljs-number">0</span>, <span class="hljs-number">50</span>, <span class="hljs-number">100</span>).reshape((<span class="hljs-number">10</span>, <span class="hljs-number">10</span>))
fig, ax = plt.subplots(ncols=<span class="hljs-number">2</span>)

cmap = plt.get_cmap(<span class="hljs-string">'RdPu'</span>)
new_cmap = truncate_colormap(cmap, <span class="hljs-number">0.3</span>, <span class="hljs-number">1</span>)

ax[<span class="hljs-number">0</span>].imshow(arr, interpolation=<span class="hljs-string">'nearest'</span>, cmap=cmap)
ax[<span class="hljs-number">1</span>].imshow(arr, interpolation=<span class="hljs-string">'nearest'</span>, cmap=new_cmap)
plt.show()
</div></code></pre>
<p>Then we choose which variables we want to overlay our map with. Here we selected Bachelors percentage, but this could be any variable we are interested in.</p>
<pre class="hljs"><code><div><span class="hljs-comment"># gdp per capita</span>
fig, ax = plt.subplots(<span class="hljs-number">1</span>, figsize=(<span class="hljs-number">12</span>,<span class="hljs-number">8</span>))
merged_df.plot(column=<span class="hljs-string">'PAD per Capita'</span>, cmap=new_cmap, linewidth=<span class="hljs-number">1</span>, ax=ax, edgecolor=<span class="hljs-string">'0.9'</span>, legend = <span class="hljs-literal">True</span>)
ax.axis(<span class="hljs-string">'off'</span>)
ax.set_title(<span class="hljs-string">'Indonesian cities/regencies by GDP Per Capita'</span>, fontdict={<span class="hljs-string">'fontsize'</span>: <span class="hljs-string">'15'</span>, <span class="hljs-string">'fontweight'</span> : <span class="hljs-string">'3'</span>})
fig.savefig(<span class="hljs-string">"Indonesia_gdp_spatial.png"</span>, dpi=<span class="hljs-number">300</span>)
</div></code></pre>
<p><img src="A training module for data-driven Public Policy in d18de1689cd5487faa0587ecf949e9ee/Indonesia_gdp_spatial.png" alt="Indonesia_gdp_spatial.png"></p>
<pre class="hljs"><code><div><span class="hljs-comment"># bachelors percentage</span>
fig, ax = plt.subplots(<span class="hljs-number">1</span>, figsize=(<span class="hljs-number">12</span>,<span class="hljs-number">8</span>))
merged_df.plot(column=<span class="hljs-string">'Bachelors_percentage'</span>, cmap=new_cmap, linewidth=<span class="hljs-number">1</span>, ax=ax, edgecolor=<span class="hljs-string">'0.9'</span>, legend = <span class="hljs-literal">True</span>)
ax.axis(<span class="hljs-string">'off'</span>)
ax.set_title(<span class="hljs-string">'Indonesian cities/regencies by Bachelors'</span>, fontdict={<span class="hljs-string">'fontsize'</span>: <span class="hljs-string">'15'</span>, <span class="hljs-string">'fontweight'</span> : <span class="hljs-string">'3'</span>})
fig.savefig(<span class="hljs-string">"Indonesia_gdp_spatial_educ_bach.png"</span>, dpi=<span class="hljs-number">300</span>)
</div></code></pre>
<p><img src="A training module for data-driven Public Policy in d18de1689cd5487faa0587ecf949e9ee/Indonesia_gdp_spatial_educ_bach.png" alt="Indonesia_gdp_spatial_educ_bach.png"></p>
<pre class="hljs"><code><div><span class="hljs-comment"># highschool percentage</span>
fig, ax = plt.subplots(<span class="hljs-number">1</span>, figsize=(<span class="hljs-number">12</span>,<span class="hljs-number">8</span>))
merged_df.plot(column=<span class="hljs-string">'high_school_percentage'</span>, cmap=new_cmap, linewidth=<span class="hljs-number">1</span>, ax=ax, edgecolor=<span class="hljs-string">'0.9'</span>, legend = <span class="hljs-literal">True</span>)
ax.axis(<span class="hljs-string">'off'</span>)
ax.set_title(<span class="hljs-string">'Indonesian cities/regencies by highschool'</span>, fontdict={<span class="hljs-string">'fontsize'</span>: <span class="hljs-string">'15'</span>, <span class="hljs-string">'fontweight'</span> : <span class="hljs-string">'3'</span>})
fig.savefig(<span class="hljs-string">"Indonesia_gdp_spatial_educ_high.png"</span>, dpi=<span class="hljs-number">300</span>)
</div></code></pre>
<p><img src="A training module for data-driven Public Policy in d18de1689cd5487faa0587ecf949e9ee/Indonesia_gdp_spatial_educ_high.png" alt="Indonesia_gdp_spatial_educ_high.png"></p>
<h2 id="interactive-visualization">Interactive visualization</h2>
<p>Within the geopandas documentation, there are examples on how to create interactive maps. We can create a map where centroids would represent the percentage of the population who have attained a bachelors degree and the color of the city represent the level of income. By doing so we can see if the level of education is a consistent determinant of a city‚Äôs income.</p>
<pre class="hljs"><code><div>merged_df[<span class="hljs-string">'kota'</span>] = merged_df.index
m = merged_df.explore(
    column = <span class="hljs-string">'PAD per Capita'</span>,
    tooltip=[<span class="hljs-string">'PAD per Capita'</span>,<span class="hljs-string">'kota'</span>,<span class="hljs-string">'Bachelors_percentage'</span>], <span class="hljs-comment"># show "BoroName" value in tooltip (on hover)</span>
    popup=<span class="hljs-literal">True</span>, <span class="hljs-comment"># show all values in popup (on click)</span>
    tiles=<span class="hljs-string">"CartoDB positron"</span>, <span class="hljs-comment"># use "CartoDB positron" tiles</span>
    scheme=<span class="hljs-string">"naturalbreaks"</span>, <span class="hljs-comment"># use "Set1" matplotlib colormap</span>
    k=<span class="hljs-number">4</span>, <span class="hljs-comment"># number of categories</span>
    name=<span class="hljs-string">'kota'</span>,
    style_kwds=dict(color=<span class="hljs-string">"black"</span>) <span class="hljs-comment"># use black outline</span>
)
folium.LayerControl().add_to(m)

<span class="hljs-comment"># get centroids to color our </span>
merged_df = test_gdf.to_crs(<span class="hljs-number">4326</span>)
merged_df[<span class="hljs-string">'lon'</span>] = test_gdf.centroid.x  
merged_df[<span class="hljs-string">'lat'</span>] = test_gdf.centroid.y
</div></code></pre>
<pre class="hljs"><code><div><span class="hljs-comment"># overlay with educ</span>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">color_producer</span><span class="hljs-params">(val)</span>:</span> <span class="hljs-comment"># mapping colors to level of education</span>
  <span class="hljs-keyword">if</span> val  &gt;= <span class="hljs-number">5</span> :
    <span class="hljs-keyword">return</span> <span class="hljs-string">'forestgreen'</span> 
  <span class="hljs-keyword">elif</span> val &gt;= <span class="hljs-number">3</span>:
    <span class="hljs-keyword">return</span> <span class="hljs-string">'yellow'</span>
  <span class="hljs-keyword">else</span>:
      <span class="hljs-keyword">return</span> <span class="hljs-string">'darkred'</span>

<span class="hljs-comment"># Add a bubble map to the base map</span>
<span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">0</span>,len(merged_df)):
    Circle(
        location=[merged_df.iloc[i][<span class="hljs-string">'lat'</span>], merged_df.iloc[i][<span class="hljs-string">'lon'</span>]],
        radius=<span class="hljs-number">20</span>,
        legend = <span class="hljs-literal">True</span>,
        color=color_producer(merged_df.iloc[i][<span class="hljs-string">'Bachelors_percentage'</span>])).add_to(m)

<span class="hljs-comment"># Display the map</span>
m
</div></code></pre>
<p><img src="A training module for data-driven Public Policy in d18de1689cd5487faa0587ecf949e9ee/Screen_Shot_2022-07-15_at_15.44.53.png" alt="Screen Shot 2022-07-15 at 15.44.53.png"></p>
<p>Lets zoom in on cities that have high income</p>
<p><img src="A training module for data-driven Public Policy in d18de1689cd5487faa0587ecf949e9ee/Screen_Shot_2022-07-15_at_15.49.13.png" alt="Screen Shot 2022-07-15 at 15.49.13.png"></p>
<p>Mimika has a relatively low bachelors percentage and yet it has extremely high income</p>
<p><img src="A training module for data-driven Public Policy in d18de1689cd5487faa0587ecf949e9ee/Screen_Shot_2022-07-15_at_15.51.59.png" alt="Screen Shot 2022-07-15 at 15.51.59.png"></p>
<p>The same is true for Manokowari Selatan. When we search what are the major sources of income for the both cities, we can see it come from natural resources. This is why, despite some of the cities not having a high level of education (inconsistent with the findings of our correlation), they had a high income. This is an important variable that we do not have in our dataset.</p>
<p>We can try our regression analysis including our outliers cities, most likely the model will perform badly and so we will remove them later.</p>
<h1 id="feature-engineering">Feature Engineering</h1>
<h2 id="mutual-information">Mutual information</h2>
<p>When dealing with data with a large amount of features it is beneficial to identify which are the potentially key features. To do so, we calculate the mutual information scores. What is measures is how much information is lost to predict/classify our target variable when a feature is removed. The advantage of using mutual information over pearson correlation is that it is not limited to identifying just linear relationships. However, with mutual information we only see how useful the variables are on their own, simply selecting variables from mutual information alone might actually be detrimental for our model performance, that said, it can give us a clear idea on what variables we know for sure are important</p>
<pre class="hljs"><code><div><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">plot_mi_scores</span><span class="hljs-params">(scores)</span>:</span>
    scores = scores.sort_values(ascending=<span class="hljs-literal">True</span>)
    width = np.arange(len(scores))
    ticks = list(scores.index)
    plt.barh(width, scores)
    plt.yticks(width, ticks)
    plt.title(<span class="hljs-string">"Mutual Information Scores"</span>)
</div></code></pre>
<pre class="hljs"><code><div><span class="hljs-comment"># mutual information</span>
X = df.copy()
X = X.drop(columns=[<span class="hljs-string">'Kabupaten/Kota'</span>],axis=<span class="hljs-number">1</span>)
y = X.pop(<span class="hljs-string">'PAD per Capita'</span>)

<span class="hljs-comment"># Label encoding for categoricals</span>
<span class="hljs-keyword">for</span> colname <span class="hljs-keyword">in</span> X.select_dtypes(<span class="hljs-string">"object"</span>):
    X[colname], _ = X[colname].factorize()

<span class="hljs-comment"># All discrete features should now have integer dtypes (double-check this before using MI!)</span>
discrete_features = X.dtypes == int
</div></code></pre>
<p>After transforming our data we can then call our functions</p>
<pre class="hljs"><code><div>mi_scores = mutual_info_regression(X, y)
mi_scores = pd.Series(mi_scores, name=<span class="hljs-string">"MI Scores"</span>, index=X.columns)
mi_scores = mi_scores.sort_values(ascending=<span class="hljs-literal">False</span>)

<span class="hljs-comment"># plot our results</span>
plt.figure(dpi=<span class="hljs-number">100</span>, figsize=(<span class="hljs-number">8</span>, <span class="hljs-number">5</span>))
plot_mi_scores(mi_scores.head(<span class="hljs-number">10</span>))
</div></code></pre>
<p><img src="A training module for data-driven Public Policy in d18de1689cd5487faa0587ecf949e9ee/Screen_Shot_2022-07-15_at_14.13.14.png" alt="Screen Shot 2022-07-15 at 14.13.14.png"></p>
<h2 id="creating-new-features">Creating new features</h2>
<p>It might very difficult and not relevant to predict the exact income of our cities. Instead we can group them by their percentiles and classify them from low income to very high income</p>
<pre class="hljs"><code><div>income_df = df[[<span class="hljs-string">'PAD per Capita'</span>,<span class="hljs-string">'Kabupaten/Kota'</span>]]
desc_table = descriptive_table(income_df) <span class="hljs-comment"># we use this are our reference to group</span>
desc_table
</div></code></pre>
<p><img src="A training module for data-driven Public Policy in d18de1689cd5487faa0587ecf949e9ee/Screen_Shot_2022-07-15_at_14.14.38.png" alt="Screen Shot 2022-07-15 at 14.14.38.png"></p>
<p>Before grouping them, we want to remove outliers from our data as they will affect our model performance. We can remove them by using the formula for outliers written below.</p>
<pre class="hljs"><code><div>iqr = desc_table[<span class="hljs-string">'75%'</span>] - desc_table[<span class="hljs-string">'25%'</span>]
outlier =  desc_table[<span class="hljs-string">'75%'</span>] + (<span class="hljs-number">1.5</span>* iqr)
print(<span class="hljs-string">'Any provinces with an income higher than'</span>,outlier,<span class="hljs-string">'will be flagged as an outlier'</span>)
</div></code></pre>
<p><code>Any provinces with an income higher than 7794.85 will be flagged as an outlier</code></p>
<p>We will also removed areas that have a income of 0 as they too are outliers</p>
<pre class="hljs"><code><div><span class="hljs-comment"># group them based on which percentiles the city income lies in </span>
income_conditions = [
    (df[<span class="hljs-string">'PAD per Capita'</span>] &lt;= <span class="hljs-number">0</span>), 
    (df[<span class="hljs-string">'PAD per Capita'</span>] &lt; float(desc_table[<span class="hljs-string">'25%'</span>])),
    (df[<span class="hljs-string">'PAD per Capita'</span>] &gt;= 	float(desc_table[<span class="hljs-string">'25%'</span>])) &amp; (df[<span class="hljs-string">'PAD per Capita'</span>] &lt;= float(desc_table[<span class="hljs-string">'75%'</span>])),
    (df[<span class="hljs-string">'PAD per Capita'</span>] &gt;= float(desc_table[<span class="hljs-string">'75%'</span>])) &amp; (df[<span class="hljs-string">'PAD per Capita'</span>] &lt; float(outlier)), 
    (df[<span class="hljs-string">'PAD per Capita'</span>] &gt;= float(outlier))]
income_categories = [<span class="hljs-string">'very low'</span>,<span class="hljs-string">'low'</span>, <span class="hljs-string">'medium'</span>, <span class="hljs-string">'high'</span>,<span class="hljs-string">'very high'</span>]
df[<span class="hljs-string">'income_categories'</span>] = np.select(income_conditions, income_categories)

<span class="hljs-comment"># check output of our transformation</span>
df[[<span class="hljs-string">'Kabupaten/Kota'</span>,<span class="hljs-string">'PAD per Capita'</span>,<span class="hljs-string">'income_categories'</span>]].tail(<span class="hljs-number">10</span>)
</div></code></pre>
<p><img src="A training module for data-driven Public Policy in d18de1689cd5487faa0587ecf949e9ee/Screen_Shot_2022-07-15_at_14.22.18.png" alt="Screen Shot 2022-07-15 at 14.22.18.png"></p>
<p>When we spatially visualized our dataset before, we saw that certain cities had high income due to a confounding variable which was natural resources. Since our dataset does not contain data for these factors, we will drop them . Other cities such as Jakarta and Badung also have very high income. This is also a special case as Jakarta is the capital city, where most of the development is concentrated in. Badung is in Bali where there are a lot of expats and high tourism leading it to have very high income revenue. There are also certain cities that depend on government transfers and have an Income of 0. These are due to special circumstances and hence we will also drop them</p>
<pre class="hljs"><code><div>df = df[df[<span class="hljs-string">"income_categories"</span>] != <span class="hljs-string">'very high'</span>]
df = df[df[<span class="hljs-string">"income_categories"</span>] != <span class="hljs-string">'very low'</span>]
</div></code></pre>
<p>We saw that from our  spatial analysis, there is correlation between income of a regency/city and its surrounding. Let us then create a new feature, get the median income for cities in each province.</p>
<pre class="hljs"><code><div><span class="hljs-comment"># get average marketcap per year</span>
df[<span class="hljs-string">"Median_income_province"</span>] = (
    df.groupby(<span class="hljs-string">"Provinsi"</span>)
    [<span class="hljs-string">"PAD per Capita"</span>]
    .transform(<span class="hljs-string">"median"</span>)
) 

df[[<span class="hljs-string">"Kabupaten/Kota"</span>, <span class="hljs-string">"Provinsi"</span>, <span class="hljs-string">"Median_income_province"</span>,<span class="hljs-string">"PAD per Capita"</span>]].head(<span class="hljs-number">5</span>)
</div></code></pre>
<p><img src="A training module for data-driven Public Policy in d18de1689cd5487faa0587ecf949e9ee/Screen_Shot_2022-07-15_at_14.28.17.png" alt="Screen Shot 2022-07-15 at 14.28.17.png"></p>
<h2 id="dealing-with-categorical-data">Dealing with categorical data</h2>
<p>In our Province column, we are dealing with categorical data. Our model cannot interpret words/strings. There are several ways to deal with this. In this situation we will be using dummy variables to represent the different provinces.</p>
<p>Essentially, what this does is we create an individual column for each province and encode it with a 1 if our sample is in a Province and 0 otherwise. By doing so, we will create 34 additional features! This may not necessarily be good for us as this creates high dimensionality. Ultimately, it will depend on our dataset. Below, we will show how to deal with categorical features nonetheless for demonstration purposes</p>
<pre class="hljs"><code><div>df = pd.get_dummies(df, columns=[<span class="hljs-string">"Provinsi"</span>]) <span class="hljs-comment"># simply select columns containing categorical data</span>
df.head()
</div></code></pre>
<p><img src="A training module for data-driven Public Policy in d18de1689cd5487faa0587ecf949e9ee/Screen_Shot_2022-07-15_at_14.30.24.png" alt="Screen Shot 2022-07-15 at 14.30.24.png"></p>
<h1 id="modeling-data">Modeling data</h1>
<p>There are many machine learning models available. In our we will use the  2 most commonly used classifier algorithms . One is Random Forest, an algorithm that combines multiple decision trees and XGBoost, which are a boosted version of Random Forest. The official documentation is accessible here:</p>
<ul>
<li>Random forest : <a href="https://scikit-learn.org/stable/modules/ensemble.html">https://scikit-learn.org/stable/modules/ensemble.html</a></li>
<li>XGBoost: <a href="https://xgboost.readthedocs.io/en/stable/">https://xgboost.readthedocs.io/en/stable/</a></li>
</ul>
<p>Before modeling our data we must first establish how we will evaluate our models</p>
<h2 id="evaluation-methods">Evaluation methods</h2>
<p>Before we continue modeling, We have to establish how to evaluate or models. This process is performed using training and test sets. Cross-validation is implemented in machine learning to prevent overfitting. This is when the dataset is divided into a training set and a test set. Typically, this is performed by allocating 75-80% of the data to the training set and the rest to the test set. The training data are used to train the models by finding the right coefficient, whereas the test set is used to test its performance on a dataset that has never been seen. This was used to verify whether the model generalizes well. Unfortunately this step often gets skipped by domain scientist that lead to unreproducible results!</p>
<p><img src="A training module for data-driven Public Policy in d18de1689cd5487faa0587ecf949e9ee/overfit.png" alt="source : towardsdatascience.com"></p>
<p>source : <a href="http://towardsdatascience.com/">towardsdatascience.com</a></p>
<h3 id="why-is-this-important">Why is this important ?</h3>
<p>Ultimately, the goal of statistical inference is to know whether our model generalizes well. What works in area may not work in other areas. For public policy, this is important for policy makers as often times they make decision based on historical events. But how do we know the same policies will be effective in other areas?</p>
<p>In the code below, call <code>train_test_split</code> which will split our dataset for us. By default it will produce a 75:25 split but we can change these values.</p>
<pre class="hljs"><code><div><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> train_test_split
<span class="hljs-keyword">from</span> sklearn.ensemble <span class="hljs-keyword">import</span> RandomForestClassifier

y = df[[<span class="hljs-string">'income_categories'</span>]]
<span class="hljs-comment">#X = X.loc[:,~X.columns.duplicated()] # certain cols were duplicates</span>
X = df.copy()
X = X.drop(columns=[<span class="hljs-string">'Kabupaten/Kota'</span>,<span class="hljs-string">'PAD per Capita'</span>,<span class="hljs-string">'income_categories'</span>],axis=<span class="hljs-number">1</span>)
train_X, val_X, train_y, val_y = 
train_test_split(X, y, random_state=<span class="hljs-number">1</span>,shuffle=<span class="hljs-literal">True</span>)

my_model = RandomForestClassifier(n_estimators=<span class="hljs-number">8000</span>,
                                 random_state=<span class="hljs-number">23</span>).fit(
																 train_X, train_y.values.ravel())
y_pred = my_model.predict(val_X)
print(<span class="hljs-string">"Accuracy:"</span>,metrics.accuracy_score(val_y, y_pred))
</div></code></pre>
<p><code>Accuracy: 0.6833333333333333</code></p>
<h3 id="xgboost">XGBoost</h3>
<pre class="hljs"><code><div>xgb_model = xgb.XGBClassifier(objective=<span class="hljs-string">'multi:softmax'</span>, n_estimators=<span class="hljs-number">50</span>,
                              learning_rate=<span class="hljs-number">0.07</span>,max_depth=<span class="hljs-number">5</span>,gamma=<span class="hljs-number">5</span>,colsample_bytree=<span class="hljs-number">0.7</span>) <span class="hljs-comment"># parameters were found using grid search.  Refer to method below</span>

xgb_model.fit(train_X, train_y.values.ravel())
y_pred = xgb_model.predict(val_X)
print(<span class="hljs-string">"Accuracy:"</span>,metrics.accuracy_score(val_y, y_pred))
</div></code></pre>
<p><code>Accuracy: 0.7</code></p>
<h2 id="grid-search--hyper-parameter-tuning">Grid search / Hyper-parameter tuning</h2>
<p>Look at the hyper-parameters inside our¬†<code>xgb.XGBClassifier()</code> How do we decide what are the ideal hyper-parameters (learning_rate, max_depth, gamma etc.)? There is no exact method to find the optimal values. Usually, finding the ideal values require trial and error, but that takes a long time. To automate this process we can use a grid search shown below. What this does is try every possible permutation to find which one produces the best result. The more options we give, the longer the process will take but the higher probability we find the optimal number. That decision depends on the time we have.</p>
<pre class="hljs"><code><div>gbm_param_grid = {
    <span class="hljs-string">'learning_rate'</span> : [<span class="hljs-number">0.1</span>],
    <span class="hljs-string">'colsample_bytree'</span>: [<span class="hljs-number">0.8</span>,<span class="hljs-number">0.7</span>,<span class="hljs-number">0.9</span>],
    <span class="hljs-string">'n_estimators'</span>: [<span class="hljs-number">60</span>,<span class="hljs-number">75</span>,<span class="hljs-number">77</span>],
    <span class="hljs-string">'max_depth'</span>: [<span class="hljs-number">7</span>,<span class="hljs-number">10</span>,<span class="hljs-number">12</span>],
    <span class="hljs-string">'gamma'</span> : [<span class="hljs-number">0.1</span>,<span class="hljs-number">0.05</span>],
    <span class="hljs-string">'objective'</span>: [<span class="hljs-string">'multi:softmax'</span>]
} 

xgb_model = xgb.XGBClassifier()
grid_acc = GridSearchCV(estimator=xgb_model,param_grid=gbm_param_grid, cv=<span class="hljs-number">2</span>, verbose=<span class="hljs-number">1</span>)
grid_acc.fit(X,y.values.ravel())

print(<span class="hljs-string">"Best parameters found: "</span>, grid_acc.best_params_)
print( <span class="hljs-string">"best score : "</span>,grid_acc.best_score_)
</div></code></pre>
<h2 id="evaluating-model-performance">Evaluating model performance</h2>
<p>Even though we have seen the accuracy of our model, when dealing with a classifier model, we want to see which classes our model is good at predicting vs which classes it struggles at predicting. By using the <code>classification_report</code> function, we can easily see this. The classification report outputs a confusion matrix which tell us how our model performed for each class ( low, medium, high) .</p>
<pre class="hljs"><code><div>class_rep = classification_report(val_y,y_pred,output_dict=<span class="hljs-literal">True</span>)
sns.heatmap(pd.DataFrame(class_rep).iloc[:<span class="hljs-number">-1</span>, :].T, annot=<span class="hljs-literal">True</span>)
</div></code></pre>
<p><img src="A training module for data-driven Public Policy in d18de1689cd5487faa0587ecf949e9ee/Screen_Shot_2022-07-15_at_14.37.51.png" alt="Screen Shot 2022-07-15 at 14.37.51.png"></p>
<p>When dealing with classification algorithms, we are dealing with multiple classes. Our model here is trying to predict which cities should be classified as low, medium and high income. To properly evaluate our model‚Äôs performance, we should evaluate our models performance for individual classes. The metric of interest is the dataset is the f1-score. From our confusion matrix, we can see that the model does a good job in predicting medium and high but struggles with low income per capita cities.</p>
<h2 id="understanding-metrics">Understanding metrics</h2>
<p>Precision : Measures how many samples that were classified in a specific category were correctly classified</p>
<p>Recall : How many of the samples of the specific category were classified</p>
<p>F1-score : harmonic mean of precision and recall</p>
<h2 id="xgboost-feature-importance">XGBoost feature importance</h2>
<p>XGBoost also allows us to see which features it deems most important, this is similar to what we did before by seeing which variables have the highest correlation to the income per capita or when we used mutual information.</p>
<pre class="hljs"><code><div>feature_names = X.columns
importances = xgb_model.feature_importances_

forest_importances = pd.Series(importances, index=feature_names)
forest_importances = forest_importances.sort_values(ascending=<span class="hljs-literal">False</span>).head(<span class="hljs-number">30</span>)

fig, ax = plt.subplots()
<span class="hljs-comment">#forest_importances.plot.bar(ax=ax)</span>
ax = sns.barplot(x=forest_importances.index, y=forest_importances.values)
plt.xticks(rotation=<span class="hljs-number">45</span>,fontsize=<span class="hljs-number">8</span>)
plt.gcf().set_size_inches(<span class="hljs-number">20</span>,<span class="hljs-number">8</span>)
ax.set_title(<span class="hljs-string">"Feature importances using MDI"</span>)
ax.set_ylabel(<span class="hljs-string">"Mean decrease in impurity"</span>)
fig.tight_layout()
</div></code></pre>
<p><img src="A training module for data-driven Public Policy in d18de1689cd5487faa0587ecf949e9ee/Screen_Shot_2022-07-15_at_14.40.19.png" alt="Screen Shot 2022-07-15 at 14.40.19.png"></p>
<h1 id="interpreting-our-model">Interpreting our model</h1>
<p>XGBoost and Random Forest have high accuracy. Unfortunately, this also comes with a trade-off which is interpretability. We cannot simply see the coefficients of each variable and how it impact the model. Fortunately for us, there are packages that deal with such problems!</p>
<h2 id="shap">SHAP</h2>
<p>Shap is a library used to explain ML models. It uses a game theory approach to see which features are most important and how each feature impacts our model. The y-axis is the features names and are arranged based on feature importance. Our x-axis shows how a value of that feature impacts the output of our model. The color of the data point indicates the range value of a particulars samples feature.</p>
<p>For more information on how shaps work, read the docs here : <a href="https://shap.readthedocs.io/en/latest/index.html">https://shap.readthedocs.io/en/latest/index.html</a></p>
<p>While by plotting feature importance was informative, we do not know how the value of each feature affects the prediction of our model. In this plot we see how our XGBoost model decides if a city gets classified in the high income category .</p>
<pre class="hljs"><code><div><span class="hljs-keyword">import</span> shap
explainer = shap.TreeExplainer(xgb_model)
shap_values = explainer.shap_values(X)
shap.initjs()

<span class="hljs-comment"># set labels for each class</span>
class_labels = [<span class="hljs-string">'high'</span>,<span class="hljs-string">'low'</span>,<span class="hljs-string">'medium'</span>] 

<span class="hljs-comment"># plot</span>
 shap.summary_plot(shap_values[<span class="hljs-number">0</span>], X.values, feature_names = X.columns)
</div></code></pre>
<p><img src="A training module for data-driven Public Policy in d18de1689cd5487faa0587ecf949e9ee/Screen_Shot_2022-07-15_at_14.42.00.png" alt="Screen Shot 2022-07-15 at 14.42.00.png"></p>
<h3 id="interpreting-shap">Interpreting SHAP</h3>
<p>Why is it so important to interpret our models in the first place?</p>
<h2 id="beware-of-data-leakage">Beware of data leakage</h2>
<p>What is data leakage? Let us say we have successfully trained a model that produces a high accuracy, great! But what if the model was able to achieve a high accuracy due to it having data that gave it an unfair advantage. For example, in our case, what if it contained data that tell us how much tax revenue a city received? Tax revenue is just the same thing as a cities income (In Indonesia a city may also receive income from city-owned enterprises). This may occur quite often when we are using a lot of data sources. This is why understanding how our model makes its predictions is important.</p>
<h2 id="understanding-shap-results">Understanding SHAP results</h2>
<p>We first can see that Median Income of a province is the main contributing factor to a cities income. There are 2 potential reasons for this.</p>
<ol>
<li>High income generating cities indirectly contributes to the surrounding cities income. People commute to higher income cities like Jakarta. Workers live in surrounding cities due to cheaper rent and property prices and spend the money they earn in the city they actually live in.</li>
<li>Cities or Regencies that benefit from natural resources tend to be clustered together. Coal mines or Gold mines may span multiple cities.</li>
</ol>
<h2 id="how-policy-makers-interpret-our-models">How  policy makers interpret our models</h2>
<h3 id="correlation-does-not-equal-causation">Correlation does not equal causation!</h3>
<p>This topic is often discussed in classical statistics, but it also holds importance even in machine learning. One of the objectives for data scientist in public policy is to distinguish between correlating variables and causality. As discussed above, this is relevant for policymakers seeking to use ML. Often, correlating variables are useful for prediction, but there is no causal relationship between them. Confounding variables can influence the correlating variables that cause spurious relationships. Being aware of such occurrences is important, and this is where domain experts can provide excellent help.</p>
<p>When looking at the results we can see some interesting data, the <strong>higher the divorce rates the more likely a city is going to be classified as high income</strong>. Should we then encouraged people to get divorced ? Probably not. There are most likely confounding factors such as in cities where divorce rates are higher, there are more job opportunities for women leading them to be less dependent on their husbands and allowing them to get divorced compared to areas where women stay in unhappy marriages because they have no other options. This probably then means that policy makers should emphasize on better and more opportunities for women in the work place especially in lower income areas.</p>
<p>It can also be difficult to determine which is the causal factor. We see that population density correlates with income per capita. Does this mean simply encouraging people to live in a city would increase the income per capita or cities with better opportunities attract more people.</p>
<p>Ultimately while the ML algorithms we employed  cannot derive causality but it can help researchers, economist and policy makers narrow down the search of what correlating factors are potentially causal factors. Having domain knowledge and discussing with domain experts can also enlighten us in interpreting the results. If a single sentence could be used as a takeaway for the whole module, it would be <strong>NOT to use ML algorithms blindly !</strong></p>
<p>Below we will show examples on how to use causal inference to understand our results.</p>
<h1 id="next-steps-how-to-derive-causality-from-our-results-brief-example">Next steps: How to derive causality from our results (brief example)</h1>
<h2 id="deriving-causality">Deriving Causality</h2>
<h3 id="graphing-relationship-between-divorce-rates-and-income">Graphing relationship between divorce rates and income</h3>
<pre class="hljs"><code><div><span class="hljs-keyword">import</span> graphviz <span class="hljs-keyword">as</span> gr
g = gr.Digraph()
g.edge(<span class="hljs-string">"Job Oppurtunites for women"</span>, <span class="hljs-string">"Divorce Rate"</span>),
g.edge(<span class="hljs-string">"Job Oppurtunites for women"</span>, <span class="hljs-string">"PAD per Capita"</span>),
g
</div></code></pre>
<p><img src="A training module for data-driven Public Policy in d18de1689cd5487faa0587ecf949e9ee/Screen_Shot_2022-07-25_at_14.18.23.png" alt="Screen Shot 2022-07-25 at 14.18.23.png"></p>
<p>Here we use Graphical Causal Models for our example of divorce rates and higher income per capita. We can see from our model how job opportunities for women is a confounding variable to divorce rates and income.</p>
<h3 id="graphing-relationship-between-pad-per-capita-income-and-natural-resources-plus-linear-regression">Graphing Relationship between PAD per Capita, Income and Natural Resources plus Linear Regression</h3>
<p>Remember when we hypothesized that natural resources was the reason why we had cities with average levels of education and yet high income. If we want to just how effective education is, we can gather data for natural resources and construct a <strong>multilinear regression model</strong> with education as the treatment, control the amount of natural resources and income per capita as the outcome. By controlling natural resources we can see the true impact of education to a cities income per capita. Could it be the cities with high levels of income have higher levels of education because their local government has more to spend on education?</p>
<pre class="hljs"><code><div>g = gr.Digraph()
g.edge(<span class="hljs-string">"Natural Resources"</span>, <span class="hljs-string">"Educ"</span>),
g.edge(<span class="hljs-string">"Natural Resources"</span>, <span class="hljs-string">"PAD per Capita"</span>),
g.edge(<span class="hljs-string">"Educ"</span>, <span class="hljs-string">"PAD per Capita"</span>)
g
</div></code></pre>
<p><img src="A training module for data-driven Public Policy in d18de1689cd5487faa0587ecf949e9ee/Screen_Shot_2022-07-25_at_14.16.25.png" alt="Screen Shot 2022-07-25 at 14.16.25.png"></p>
<p>The formula to construct our linear regression model would then be:
<img src="A training module for data-driven Public Policy in d18de1689cd5487faa0587ecf949e9ee/latex.png" alt="latex.png"></p>
<h2 id="using-instrumental-variables">Using Instrumental Variables</h2>
<p>Another advantage of using XGBoost or Random Forest (RF) is that it can help identify Instrumental Variables(IV). IV‚Äôs are often used in the field of econometrics to help estimate causal relationships when Randomized Controlled Trials (RCT) are not possible. As we know, confounding variables can greatly influence the results of our regression. While adding confounding variables is the most optimal solution, oftentimes this is not possible. We can substitute these confounding variables with IV‚Äôs.¬† IV‚Äôs are variables that are <em>correlated to the treatment given but have no direct correlation to the outcome variable</em>.¬† Identifying IV‚Äôs are often challenging, however can they be more easily derived from the feature importance generated by the RF or XGBoost model we have. It needs to be stressed that selecting IV needs to be justified with qualitative and this requires domain knowledge.</p>
<p><img src="A training module for data-driven Public Policy in d18de1689cd5487faa0587ecf949e9ee/image.png" alt="image.png">
data source : Detecting Heterogeneous Treatment Effect with Instrumental Variables - Scientific Figure on ResearchGate[10]</p>
<h2 id="recommended-reading">Recommended Reading</h2>
<p>For public policy, understanding causality is vital. In the final section, we have seen how we can bridge Data science techniques and ML with causal inference. A great in-depth guide on causal inference can be found here : <a href="https://matheusfacure.github.io/python-causality-handbook/landing-page.html">https://matheusfacure.github.io/python-causality-handbook/landing-page.html</a></p>
<h1 id="suggestions-to-the-indonesian-government-for-database-design">Suggestions to the Indonesian Government for Database design</h1>
<p>A large percentage of the workflow was not thoroughly covered in this module. They were data acquisition and data cleaning. While the government's current initiative was to have a single repository of data stored in Satu Data Indonesia, the data used in this module was manually scraped from the Website of the Ministry of Finance and from the Civil Registry Service Office. Web Scraping is not only a tedious process but can be imperfect which may lead to spurious results. Having a functioning SQL database can easily alleviate this problem as data would be directly queried from the source.</p>
<p>Another problem that arose was the lack of database documentation. A well maintained database such the one created by The Department for Digital, Culture, Media and Sport is a department of His Majesty's Government is always accompanied by a Meta Data that elaborates a thorough summary of the contents of each table in the database and¬† explanation of each column. This not only accelerates but improves the analysis that can and will be made by stakeholders(Data Analysts, Economists. Data scientists, Statisticians etc.) as this minimizes the time spent needed to understand the content.</p>
<p>Finally a well-designed database would utilize the use of Foreign Keys(FK). There is often discrepancy between different departments when referencing the same data. This may unintentionally lead to spurious regression.  Usage of FK would enforce data integrity as different departments can no longer have conflicting data.</p>
<p>Another Benefit of FK would also be the Uniformity of Variable naming conventions. Instead of referencing cities/ provinces which different departments often use different naming conventions, using numerical IDs will ensure uniformity.</p>
<p>In summary, thorough documentation, Migrating data to a SQL database and using FK will increase workflow, reduce false results and most importantly increase transparency. While this will initially take a significant amount of time to implement, it is the gold-standard. The dividends reaped by adopting this more robust design will significantly outweigh the cost.</p>
<h1 id="appendix">Appendix</h1>
<h2 id="reproducible-environments">Reproducible environments</h2>
<p>Installing packages and libraries in python can often be problematic. Versions of packages may not be compatible and create dependencies error. To minimize time solving these issues we must create reproducible environments. There are several ways to achieve this but using <code>.yml</code>files are an easy way to do so.</p>
<p>For this code notebook download the download the¬†<code>.yml</code> file from here¬†<a href="https://drive.google.com/file/d/1aLexkh93ajIeRVo-C7zSMPqbR37h7aiU/view?usp=sharing">https://drive.google.com/file/d/1aLexkh93ajIeRVo-C7zSMPqbR37h7aiU/view?usp=sharing</a></p>
<p>Once this is downloaded open your conda terminal and create  the environment</p>
<pre class="hljs"><code><div>conda env create -f filename.yml 
</div></code></pre>
<p>Then activate it</p>
<pre class="hljs"><code><div>conda activate <span class="hljs-string">'enviromentname'</span> 
</div></code></pre>
<h2 id="exporting-your-environment">Exporting your environment</h2>
<p>If we want others to use our environment we can export our .yml file</p>
<p>To do so ensure the environement you want to export is activated</p>
<pre class="hljs"><code><div>conda activate enviromentname <span class="hljs-comment"># remove the parenthesis </span>
</div></code></pre>
<p>Then export it</p>
<pre class="hljs"><code><div>conda env <span class="hljs-built_in">export</span> &gt; filename.yml
</div></code></pre>
<p>This file can then be shared to anyone else that would like to use the environment</p>
<h2 id="additional-ways-to-improve-our-model-performance">Additional ways to improve our model performance</h2>
<p>As mentioned before, removing features may actually improve our model performance. This all falls under dimensionality reduction. High dimension data is when there are a lot of independent variables in comparison to our number of data points. For the most part however, it usually just speeds up training and inference time of our model.</p>
<h2 id="dimensionality-reduction-techniques-include">Dimensionality reduction techniques include</h2>
<ol>
<li>Recursive elimination</li>
<li>PCA</li>
</ol>
<p>Recursive elimination : <a href="https://machinelearningmastery.com/rfe-feature-selection-in-python/">https://machinelearningmastery.com/rfe-feature-selection-in-python/</a></p>
<p>PCA : <a href="https://www.simplilearn.com/tutorials/machine-learning-tutorial/principal-component-analysis">https://www.simplilearn.com/tutorials/machine-learning-tutorial/principal-component-analysis</a></p>
<h2 id="citations">Citations</h2>
<ol>
<li>Amarasinghe, Kasun, et al. &quot;Explainable. machine learning for public policy: Use cases, gaps, and research directions.&quot;¬†<em>arXiv preprint arXiv:2010.14374</em>¬†(2020).</li>
<li>Badan Pusat Stastik.¬†<em>Analysis Of Current Issue</em>. BPS-Statistics Indonesia, 2021, pp. 10 - 37, 53.</li>
<li>F W Wibowo and Wihayati 2021 J. Phys.: Conf. Ser. 1844 012006</li>
<li>Leonita, G.; Kuffer, M.; Sliuzas, R.; Persello, C. Machine Learning-Based Slum Mapping in Support of Slum Upgrading Programs: The Case of Bandung City, Indonesia.¬†<em>Remote Sens.</em>¬†<strong>2018</strong>,¬†<em>10</em>, 1522. <a href="https://doi.org/10.3390/rs10101522">https://doi.org/10.3390/rs10101522</a></li>
<li>Kaufman, Shachar, Saharon Rosset, Claudia Perlich, and Ori Stitelman. &quot;Leakage in data mining: Formulation, detection, and avoidance.&quot;¬†<em>ACM Transactions on Knowledge Discovery from Data (TKDD)</em>¬†6, no. 4 (2012): 1-21.</li>
<li>****Head ML, Holman L, Lanfear R, Kahn AT, Jennions MD (2015) The Extent and Consequences of P-Hacking in Science. PLoS Biol 13(3): e1002106. <a href="https://doi.org/10.1371/journal.pbio.1002106">https://doi.org/10.1371/journal.pbio.1002106</a></li>
<li>Kreif, Noemi, Karla DiazOrdaz, Rodrigo Moreno-Serra, Andrew Mirelman, Taufik Hidayat, and Marc Suhrcke. &quot;Estimating heterogeneous policy impacts using causal machine learning: a case study of health insurance reform in Indonesia.&quot;¬†<em>Health Services and Outcomes Research Methodology</em>¬†(2021): 1-36.</li>
<li>Molnar, Christoph. ‚ÄúInterpretable machine learning. A Guide for Making Black Box Models Explainable‚Äù, 2019.¬†<a href="https://christophm.github.io/interpretable-ml-book/">https://christophm.github.io/interpretable-ml-book/</a>.</li>
<li>‚ÄúAnscombe's quartet ‚Äî Matplotlib 3.5.2 documentation.‚Äù <em>Matplotlib</em>, https://matplotlib.org/stable/gallery/specialty_plots/anscombe.html. Accessed 21 July 2022.</li>
<li>Detecting Heterogeneous Treatment Effect with Instrumental Variables - Scientific Figure on ResearchGate. Available from: https://www.researchgate.net/figure/Diagram-of-instrumental-variable-assumptions_fig1_335135070 Accessed 25 Dec, 2022</li>
</ol>
<h3 id="data-used-and-documentation">Data Used and Documentation</h3>
<table>
<thead>
<tr>
<th>Variable Name</th>
<th>Type</th>
<th>Defintion/ English Translation</th>
<th>Data Source</th>
<th>Comments</th>
</tr>
</thead>
<tbody>
<tr>
<td>Provinsi</td>
<td>Categorical</td>
<td>Name of province</td>
<td>Department of Population and Civil Registration</td>
<td></td>
</tr>
<tr>
<td>Kabupaten/Kota</td>
<td>Categorical</td>
<td>Name of city or regency</td>
<td>Department of Population and Civil Registration</td>
<td></td>
</tr>
<tr>
<td>Jumlah Kecamatan</td>
<td>Continuous</td>
<td>Number of districts</td>
<td>Department of Population and Civil Registration</td>
<td></td>
</tr>
<tr>
<td>Jumlah Desa</td>
<td>Continuous</td>
<td>Number of villages</td>
<td>Department of Population and Civil Registration</td>
<td></td>
</tr>
<tr>
<td>Jumlah Kelurahan</td>
<td>Continuous</td>
<td>Number of wards</td>
<td>Department of Population and Civil Registration</td>
<td></td>
</tr>
<tr>
<td>Jumlah Penduduk</td>
<td>Continuous</td>
<td>Number of population</td>
<td>Department of Population and Civil Registration</td>
<td></td>
</tr>
<tr>
<td>Jumlah KK</td>
<td>Continuous</td>
<td>Number of families</td>
<td>Department of Population and Civil Registration</td>
<td></td>
</tr>
<tr>
<td>Luas Wilayah (km2)</td>
<td>Continuous</td>
<td>Size of Area</td>
<td>Department of Population and Civil Registration</td>
<td></td>
</tr>
<tr>
<td>Kepadatan Penduduk</td>
<td>Continuous</td>
<td>Population Density</td>
<td>Department of Population and Civil Registration</td>
<td></td>
</tr>
<tr>
<td>Perpindahan Penduduk</td>
<td>Continuous</td>
<td>Migration</td>
<td>Department of Population and Civil Registration</td>
<td></td>
</tr>
<tr>
<td>Jumlah Meninggal</td>
<td>Continuous</td>
<td>Number of deaths</td>
<td>Department of Population and Civil Registration</td>
<td></td>
</tr>
<tr>
<td>Wajib KTP</td>
<td>Continuous</td>
<td>Number with an ID</td>
<td>Department of Population and Civil Registration</td>
<td></td>
</tr>
<tr>
<td>Laki-Laki</td>
<td>Continuous</td>
<td>Percentage of men</td>
<td>Department of Population and Civil Registration</td>
<td></td>
</tr>
<tr>
<td>Perempuan</td>
<td>Continuous</td>
<td>Percentage of women</td>
<td>Department of Population and Civil Registration</td>
<td></td>
</tr>
<tr>
<td>Belum Kawin</td>
<td>Continuous</td>
<td>percentage of unmarried people</td>
<td>Department of Population and Civil Registration</td>
<td></td>
</tr>
<tr>
<td>Kawin</td>
<td>Continuous</td>
<td>Percentage of married people</td>
<td>Department of Population and Civil Registration</td>
<td></td>
</tr>
<tr>
<td>Cerai Hidup</td>
<td>Continuous</td>
<td>Number of divorced</td>
<td>Department of Population and Civil Registration</td>
<td></td>
</tr>
<tr>
<td>Cerai Mati</td>
<td>Continuous</td>
<td>Number of divorced due to death</td>
<td>Department of Population and Civil Registration</td>
<td></td>
</tr>
<tr>
<td>Usia 0-4 thn</td>
<td>Continuous</td>
<td>Percentage of those between ages 0 - 4</td>
<td>Department of Population and Civil Registration</td>
<td></td>
</tr>
<tr>
<td>Usia 5-9 thn</td>
<td>Continuous</td>
<td>Percentage of those between ages 5 - 9</td>
<td>Department of Population and Civil Registration</td>
<td></td>
</tr>
<tr>
<td>Usia 10-14 thn</td>
<td>Continuous</td>
<td>Percentage of those between ages 10 - 10</td>
<td>Department of Population and Civil Registration</td>
<td></td>
</tr>
<tr>
<td>Usia 15-19 thn</td>
<td>Continuous</td>
<td>Percentage of those between ages 15 - 11</td>
<td>Department of Population and Civil Registration</td>
<td></td>
</tr>
<tr>
<td>Usia 20-24 thn</td>
<td>Continuous</td>
<td>Percentage of those between ages 20 - 12</td>
<td>Department of Population and Civil Registration</td>
<td></td>
</tr>
<tr>
<td>Usia 25-29 thn</td>
<td>Continuous</td>
<td>Percentage of those between ages 25 - 13</td>
<td>Department of Population and Civil Registration</td>
<td></td>
</tr>
<tr>
<td>Usia 30-34 thn</td>
<td>Continuous</td>
<td>Percentage of those between ages 30 - 14</td>
<td>Department of Population and Civil Registration</td>
<td></td>
</tr>
<tr>
<td>Usia 35-39 thn</td>
<td>Continuous</td>
<td>Percentage of those between ages 35 - 15</td>
<td>Department of Population and Civil Registration</td>
<td></td>
</tr>
<tr>
<td>Usia 40-44 thn</td>
<td>Continuous</td>
<td>Percentage of those between ages 40 - 16</td>
<td>Department of Population and Civil Registration</td>
<td></td>
</tr>
<tr>
<td>Usia 45-49 thn</td>
<td>Continuous</td>
<td>Percentage of those between ages 45 - 17</td>
<td>Department of Population and Civil Registration</td>
<td></td>
</tr>
<tr>
<td>Usia 50-54 thn</td>
<td>Continuous</td>
<td>Percentage of those between ages 50 - 18</td>
<td>Department of Population and Civil Registration</td>
<td></td>
</tr>
<tr>
<td>Usia 55-59 thn</td>
<td>Continuous</td>
<td>Percentage of those between ages 55 - 19</td>
<td>Department of Population and Civil Registration</td>
<td></td>
</tr>
<tr>
<td>Usia 60-64 thn</td>
<td>Continuous</td>
<td>Percentage of those between ages 60 - 20</td>
<td>Department of Population and Civil Registration</td>
<td></td>
</tr>
<tr>
<td>Usia 65-69 thn</td>
<td>Continuous</td>
<td>Percentage of those between ages 65 - 21</td>
<td>Department of Population and Civil Registration</td>
<td></td>
</tr>
<tr>
<td>Usia 70-74 thn</td>
<td>Continuous</td>
<td>Percentage of those between ages 70 - 22</td>
<td>Department of Population and Civil Registration</td>
<td></td>
</tr>
<tr>
<td>Usia 75 thn ke Atas</td>
<td>Continuous</td>
<td>Percentage of those above 75</td>
<td>Department of Population and Civil Registration</td>
<td></td>
</tr>
<tr>
<td>Lahir thn 2018</td>
<td>Continuous</td>
<td>Born after 2018</td>
<td>Department of Population and Civil Registration</td>
<td></td>
</tr>
<tr>
<td>Lahir sebelum thn 2018</td>
<td>Continuous</td>
<td>Born berfore 2018</td>
<td>Department of Population and Civil Registration</td>
<td></td>
</tr>
<tr>
<td>Pertumbuhan penduduk thn 2016 (%)</td>
<td>Continuous</td>
<td>Change in population in 2016</td>
<td>Department of Population and Civil Registration</td>
<td></td>
</tr>
<tr>
<td>Pertumbuhan penduduk thn 2017 (%)</td>
<td>Continuous</td>
<td>Change in population in 2017</td>
<td>Department of Population and Civil Registration</td>
<td></td>
</tr>
<tr>
<td>Pertumbuhan penduduk thn 2018 (%)</td>
<td>Continuous</td>
<td>Change in population in 2018</td>
<td>Department of Population and Civil Registration</td>
<td></td>
</tr>
<tr>
<td>Pertumbuhan penduduk thn 2019 (%)</td>
<td>Continuous</td>
<td>Change in population in 2019</td>
<td>Department of Population and Civil Registration</td>
<td></td>
</tr>
<tr>
<td>Pertumbuhan penduduk thn 2020 (%)</td>
<td>Continuous</td>
<td>Change in population in 2020</td>
<td>Department of Population and Civil Registration</td>
<td></td>
</tr>
<tr>
<td>Belum/Tidak Bekerja</td>
<td>Continuous</td>
<td>Percentage of No/ No Job yet</td>
<td>Department of Population and Civil Registration</td>
<td></td>
</tr>
<tr>
<td>Aparatur Pejabat Negara</td>
<td>Continuous</td>
<td>Percentage of Government official</td>
<td>Department of Population and Civil Registration</td>
<td></td>
</tr>
<tr>
<td>Tenaga Pengajar</td>
<td>Continuous</td>
<td>Percentage of school workforce</td>
<td>Department of Population and Civil Registration</td>
<td></td>
</tr>
<tr>
<td>Wiraswasta</td>
<td>Continuous</td>
<td>Percentage Entrepeneurs</td>
<td>Department of Population and Civil Registration</td>
<td></td>
</tr>
<tr>
<td>Pertanian dan Peternakan</td>
<td>Continuous</td>
<td>Percentage in Agriculture or farming</td>
<td>Department of Population and Civil Registration</td>
<td></td>
</tr>
<tr>
<td>Nelayan</td>
<td>Continuous</td>
<td>Percentage of fisherman</td>
<td>Department of Population and Civil Registration</td>
<td></td>
</tr>
<tr>
<td>Pelajar dan Mahasiswa</td>
<td>Continuous</td>
<td>Percentage of Students</td>
<td>Department of Population and Civil Registration</td>
<td></td>
</tr>
<tr>
<td>Pensiunan</td>
<td>Continuous</td>
<td>Percentage of Retired</td>
<td>Department of Population and Civil Registration</td>
<td></td>
</tr>
<tr>
<td>Pekerjaan Lainnya</td>
<td>Continuous</td>
<td>Other Jobs</td>
<td>Department of Population and Civil Registration</td>
<td></td>
</tr>
<tr>
<td>f4_18_tahun_pendidikan_khusus</td>
<td>Continuous</td>
<td>Need to search or remove</td>
<td>Department of Population and Civil Registration</td>
<td></td>
</tr>
<tr>
<td>f5_6_tahun_paud</td>
<td>Continuous</td>
<td>Need to search or remove</td>
<td>Department of Population and Civil Registration</td>
<td></td>
</tr>
<tr>
<td>f7_12_tahun_sd</td>
<td>Continuous</td>
<td>Need to search or remove</td>
<td>Department of Population and Civil Registration</td>
<td></td>
</tr>
<tr>
<td>f12_15_tahun_smp</td>
<td>Continuous</td>
<td>Need to search or remove</td>
<td>Department of Population and Civil Registration</td>
<td></td>
</tr>
<tr>
<td>f16_18_tahun_sma</td>
<td>Continuous</td>
<td>Need to search or remove</td>
<td>Department of Population and Civil Registration</td>
<td></td>
</tr>
<tr>
<td>lahir_thn4</td>
<td>Continuous</td>
<td>Need to search or remove</td>
<td>Department of Population and Civil Registration</td>
<td></td>
</tr>
<tr>
<td>lahir_thn5</td>
<td>Continuous</td>
<td>Need to search or remove</td>
<td>Department of Population and Civil Registration</td>
<td></td>
</tr>
<tr>
<td>lahir_thn6</td>
<td>Continuous</td>
<td>Need to search or remove</td>
<td>Department of Population and Civil Registration</td>
<td></td>
</tr>
<tr>
<td>lahir_seb4</td>
<td>Continuous</td>
<td>Need to search or remove</td>
<td>Department of Population and Civil Registration</td>
<td></td>
</tr>
<tr>
<td>lahir_seb5</td>
<td>Continuous</td>
<td>Need to search or remove</td>
<td>Department of Population and Civil Registration</td>
<td></td>
</tr>
<tr>
<td>lahir_seb6</td>
<td>Continuous</td>
<td>Need to search or remove</td>
<td>Department of Population and Civil Registration</td>
<td></td>
</tr>
<tr>
<td>jml_rek_wktp</td>
<td>Continuous</td>
<td>Need to search or remove</td>
<td>Department of Population and Civil Registration</td>
<td></td>
</tr>
<tr>
<td>percentage_Islam</td>
<td>Continuous</td>
<td>Percentage of Muslims</td>
<td>Department of Population and Civil Registration</td>
<td></td>
</tr>
<tr>
<td>percentage_Kristen</td>
<td>Continuous</td>
<td>Percentage of Christians</td>
<td>Department of Population and Civil Registration</td>
<td></td>
</tr>
<tr>
<td>percentage_Katholik</td>
<td>Continuous</td>
<td>Percentage of Katholik</td>
<td>Department of Population and Civil Registration</td>
<td></td>
</tr>
<tr>
<td>percentage_Hindu</td>
<td>Continuous</td>
<td>Percentage of Hindhus</td>
<td>Department of Population and Civil Registration</td>
<td></td>
</tr>
<tr>
<td>percentage_Budha</td>
<td>Continuous</td>
<td>Percentage of Budhist</td>
<td>Department of Population and Civil Registration</td>
<td></td>
</tr>
<tr>
<td>percentage_Konghucu</td>
<td>Continuous</td>
<td>Percentage of Confucianist</td>
<td>Department of Population and Civil Registration</td>
<td></td>
</tr>
<tr>
<td>percentage_no/havent_school</td>
<td>Continuous</td>
<td>Percentage of no school</td>
<td>Department of Population and Civil Registration</td>
<td></td>
</tr>
<tr>
<td>Not_finished_elementary</td>
<td>Continuous</td>
<td>Percentage of population who have not finished elementary school</td>
<td>Department of Population and Civil Registration</td>
<td></td>
</tr>
<tr>
<td>elementary_school_percentage</td>
<td>Continuous</td>
<td>Percentage of population in elementary school</td>
<td>Department of Population and Civil Registration</td>
<td></td>
</tr>
<tr>
<td>middle_school_percentage</td>
<td>Continuous</td>
<td>Percentage in middle school</td>
<td>Department of Population and Civil Registration</td>
<td></td>
</tr>
<tr>
<td>high_school_percentage</td>
<td>Continuous</td>
<td>percentage of people in highschool</td>
<td>Department of Population and Civil Registration</td>
<td></td>
</tr>
<tr>
<td>vocational_school_percentage</td>
<td>Continuous</td>
<td>percentage of people in vocational school</td>
<td>Department of Population and Civil Registration</td>
<td></td>
</tr>
<tr>
<td>highervocational_school_percentage</td>
<td>Continuous</td>
<td>Percentage of people in higher vocational school</td>
<td>Department of Population and Civil Registration</td>
<td></td>
</tr>
<tr>
<td>Bachelors_percentage</td>
<td>Continuous</td>
<td>Percentage of people with a Bachelors degree</td>
<td>Department of Population and Civil Registration</td>
<td></td>
</tr>
<tr>
<td>Masters_percentage</td>
<td>Continuous</td>
<td>Percentage of people with a Masters degree</td>
<td>Department of Population and Civil Registration</td>
<td></td>
</tr>
<tr>
<td>Phd_percentage</td>
<td>Continuous</td>
<td>Percentage of people with a Phd degree</td>
<td>Department of Population and Civil Registration</td>
<td></td>
</tr>
<tr>
<td>Non_Islam</td>
<td>Continuous</td>
<td>Percentage of non Muslims</td>
<td>Department of Population and Civil Registration</td>
<td></td>
</tr>
<tr>
<td>PAD per Capita</td>
<td>Continuous</td>
<td>Actual income per capita of a city or regency. City or regencies can recieve money transfer from the central government to assist them.</td>
<td>Ministry of Finance</td>
<td></td>
</tr>
<tr>
<td>WADMKK</td>
<td>Categorical</td>
<td>Name of cities</td>
<td>Geospatial Information Agency</td>
<td>data for geospatial visualization</td>
</tr>
<tr>
<td>geometry</td>
<td>MULTIPOLYGON</td>
<td>Multipolygon ( Shapes of the cities)</td>
<td>Geospatial Information Agency</td>
<td>data for geospatial visualization</td>
</tr>
</tbody>
</table>

</body>
</html>
